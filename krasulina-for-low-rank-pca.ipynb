{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy rights @Cheng Tang chengtang48@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan for numerical simulations\n",
    "\n",
    "- low-rank data generation: high dimensional, from a subset of coordinates (these are eigenvectors)\n",
    "- krasulina implementation (done)\n",
    "- TODO:\n",
    "    - investigate the effect of dimension and rank\n",
    "    - investigate perturbed version (optional)\n",
    "    - compare to Ohad Shamir's SVRG variant of Oja's method (optional)\n",
    "- Simulation on real data (candidates: MNIST, CCAT?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pseodo-code for generalized krasulina\n",
    "``` \n",
    "Input params: init_weights (W^0) (k by d), learning rate schedule (constant and inverse-time), max_iter (T)\n",
    "While t <= T\n",
    " W^{t+1} = W^t + \\eta^t W^t x (x - (W^t)^TW^tx)^T\n",
    " The step above is equivalent to one SGD update (given that W^t is row orthonormalized) on the objective:\n",
    " \n",
    " row orthonormalize W^{t+1}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tcheng/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/Users/tcheng/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generators: data should be centered and dispersed around the subspace spanned by gt\n",
    "\n",
    "def get_train_data(n_train, groundtruth=None):\n",
    "    \"\"\"\n",
    "    gt: d-dimensional vec (non-increasing entries): \n",
    "    nonzero at coordinates corresponding to eigenvectors, and the value corresponds eigenvalue\n",
    "    example: [1,1,0,0,0]\n",
    "    return: np array train data\n",
    "    \"\"\"\n",
    "    if not groundtruth:\n",
    "        print(\"The data generator without groundtruth is not implemented!\")\n",
    "        return\n",
    "    else:\n",
    "        train_data = None\n",
    "        d = len(groundtruth)\n",
    "        for _ in range(n_train):\n",
    "            x = np.zeros([1, d])\n",
    "            for i in range(d):\n",
    "                if groundtruth[i] > 0 :\n",
    "                    x[:, i] = np.random.normal(0, groundtruth[i]**0.5)\n",
    "                else:\n",
    "                    break\n",
    "            if train_data is None:\n",
    "                train_data = x\n",
    "            else:\n",
    "                train_data = np.concatenate((train_data, x), axis=0)  \n",
    "            #print(train_data.shape)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = get_train_data(5000, groundtruth=[1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test get_train_data\n",
    "# for i in range(2):\n",
    "#     ax_mean = np.mean(train_data[:, i])\n",
    "#     ax_var = np.var(train_data[:, i])\n",
    "#     print(f\"axis {i} mean {ax_mean}\")\n",
    "#     print(f\"axis {i} var {ax_var}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KrasulinaPCA(object):\n",
    "    \n",
    "    def __init__(self, init_weights, train_data, groundtruth=None,\n",
    "                 learning_rate=[None, 0.0001], mini_batch_size=1, max_iter=100):\n",
    "        \n",
    "        self._init_weights = init_weights\n",
    "        #self._weights = tf.Variable(init_weights, dtype=tf.float64)\n",
    "        self._train_data = train_data\n",
    "        #assert learning_rate[0] is None, \"Decaying learning rate not implemented yet!\"\n",
    "        self._learning_rate = learning_rate\n",
    "        #self._minibatch_iter = 0\n",
    "        #assert groundtruth, \"training with real data is not implemented yet!\"\n",
    "        self._groundtruth = groundtruth\n",
    "        self._k, self._d = init_weights.shape[0], init_weights.shape[1]\n",
    "        self._mbsize = mini_batch_size\n",
    "        self._T = max_iter\n",
    "        self._train_mse_log = list()\n",
    "            \n",
    "    def _get_sgd_train_op(self):\n",
    "        #train_data_placeholder = tf.placeholder(tf.float64, [self._mbsize, self._d])\n",
    "        return tf.train.GradientDescentOptimizer(self._learning_rate).minimize(\n",
    "                                                mse_loss_op(self._mini_batch_placeholder, \n",
    "                                                self._weights))\n",
    "    \n",
    "    def _get_mini_batch_placeholder(self):\n",
    "        return tf.placeholder(tf.float64, [self._mbsize, self._d])\n",
    "    \n",
    "    def _set_train_graph(self):\n",
    "        \"\"\"\n",
    "        Define tf graph\n",
    "        \"\"\"\n",
    "        # tf variables\n",
    "        self._weights = tf.Variable(self._init_weights, dtype=tf.float64)\n",
    "        c0, t0 = self._learning_rate\n",
    "        if c0 is None:\n",
    "            #signals that learning_rate is constant\n",
    "            self._learning_rate = tf.Variable(t0, trainable=False)\n",
    "            self._constant_lr = True\n",
    "        else:\n",
    "            # TODO: set inverse-decay schedule\n",
    "            lr_init = c0 / math.log(t0)\n",
    "            print(f\"Using inverse decay learning rate with initial learning rate {lr_init}\")\n",
    "            self._global_step = tf.Variable(t0, trainable=False)\n",
    "            self._increment_step_op = tf.assign(self._global_step, self._global_step+1)\n",
    "            self._learning_rate = c0 / tf.log(tf.to_float(self._global_step))\n",
    "            #self._learning_rate = tf.Variable(learn_rate, trainable=False)\n",
    "            self._constant_lr = False\n",
    "        \n",
    "        # tf data placeholder\n",
    "        self._mini_batch_placeholder = self._get_mini_batch_placeholder()\n",
    "        \n",
    "        # tf ops\n",
    "        self._train_op = self._get_sgd_train_op()\n",
    "        self._ortho_op = get_row_orthonormalize_op(self._weights)\n",
    "        #print(self._ortho_op, self._weights)\n",
    "            \n",
    "        \n",
    "    def _train(self):\n",
    "        tf.reset_default_graph()\n",
    "        # create train log for objective loss (mse)\n",
    "        self._train_mse_log.append(eval_mse_loss(self._train_data, self._init_weights))\n",
    "        #if self._groundtruth:\n",
    "            # create another log for groundtruth metric \n",
    "        self._groundtruth_eval_log = list()\n",
    "        self._groundtruth_eval_log.append(eval_with_groundtruth(self._groundtruth, self._init_weights))\n",
    "        print(f\"The initial mse: {self._train_mse_log[0]}\")\n",
    "        print(f\"The initial loss: {self._groundtruth_eval_log[0]}\")\n",
    "        self._set_train_graph()\n",
    "        mb_iter = 0\n",
    "        self._epoch = 0\n",
    "        ## start training\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())            \n",
    "            for t in range(1, self._T):\n",
    "                mini_batch, mb_iter = self._get_mini_batch(mb_iter)\n",
    "                # run optimization\n",
    "                sess.run([self._train_op] , feed_dict={self._mini_batch_placeholder: mini_batch})\n",
    "                if self._constant_lr:\n",
    "                    ## row-normalization of updated weights\n",
    "                    _, _weights = sess.run([self._ortho_op, self._weights]) \n",
    "                else:\n",
    "                    _, _, _weights = sess.run([self._increment_step_op, self._ortho_op, self._weights])  \n",
    "                \"\"\"\n",
    "                TODO: check that rows of _weights are orthonormal (difference with eye should be small)\n",
    "                \"\"\" \n",
    "                ## logging\n",
    "                self._train_mse_log.append(eval_mse_loss(self._train_data, _weights))\n",
    "                \n",
    "                self._groundtruth_eval_log.append(eval_with_groundtruth(self._groundtruth, _weights))\n",
    "        \n",
    "    \n",
    "    def _reset_train_logs(self):\n",
    "        self._train_log = list()\n",
    "        self._groundtruth_eval_log = list()\n",
    "    \n",
    "    \n",
    "    def _get_mini_batch(self, mb_iter):\n",
    "        if mb_iter + self._mbsize < len(self._train_data):\n",
    "            mini_batch = self._train_data[mb_iter : mb_iter+self._mbsize, :]\n",
    "            mb_iter_new = mb_iter + self._mbsize\n",
    "        else:\n",
    "            self._epoch += 1\n",
    "            print(f\"Finished training {self._epoch}-th epoch\")\n",
    "            mb_iter_new = (mb_iter + self._mbsize) % (len(train_data)-1) - 1\n",
    "            mini_batch_1 = self._train_data[mb_iter :, :]\n",
    "            mini_batch_2 = self._train_data[: mb_iter_new, :]\n",
    "            if mini_batch_2.size > 0:\n",
    "                mini_batch = mini_batch_1\n",
    "            else:\n",
    "                mini_batch = np.concatenate((mini_batch_1, mini_batch_2), axis=0)\n",
    "        return mini_batch, mb_iter_new\n",
    "        \n",
    "              \n",
    "def mse_loss_op(data_placeholder, weights):\n",
    "    \"\"\"\n",
    "    Input: tf place holder, data_placeholder: n by d, tf variable, weights: k by d\n",
    "    \"\"\"\n",
    "    #print(\"minibatch shape\",data_placeholder.get_shape()[0].value, data_placeholder.get_shape()[1].value)\n",
    "    encoder = tf.matmul(data_placeholder, weights, transpose_b=True)\n",
    "    decoder = tf.matmul(encoder, weights)\n",
    "    #print(\"decoder shape\", decoder.get_shape()[0].value, decoder.get_shape()[1].value)\n",
    "    return tf.reduce_sum(tf.square(data_placeholder - decoder))\n",
    "\n",
    "\n",
    "def get_row_orthonormalize_op(weights):\n",
    "    \"\"\"\n",
    "    Description: implements Gram-schmidt on rows of the tf weights matrix\n",
    "    Input: weights as a tf variable: k by d (k <= d)\n",
    "    Return: orthonormalize operation in tf graph\n",
    "    \"\"\"\n",
    "    assert weights.get_shape()[0].value <= weights.get_shape()[1].value, \"k cannot exceed d!\"\n",
    "    # add batch dimension for matmul\n",
    "    ortho_weights = tf.expand_dims(weights[0,:]/tf.norm(weights[0,:]),0)\n",
    "    for i in range(1, weights.get_shape()[0].value):\n",
    "        v = weights[i,:]\n",
    "        # add batch dimension for matmul\n",
    "        v = tf.expand_dims(v, 0) \n",
    "        r = v - tf.matmul(tf.matmul(v, ortho_weights, transpose_b=True), ortho_weights)\n",
    "        ortho_weights = tf.concat([ortho_weights, r/tf.norm(r)], axis=0)\n",
    "    return tf.assign(weights, ortho_weights)\n",
    "\n",
    "\n",
    "def eval_with_groundtruth(groundtruth, weights):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        groundtruth, list in the form [2, 1.4, 1, 0, 0]\n",
    "        weights, np array, shape k by d (row-orthonormal)\n",
    "    Return:\n",
    "        tr(U(I-P)): U-projection matrix of eigenvectors, \n",
    "                    P: projection matrix of weights\n",
    "    \"\"\"\n",
    "    #eigenvecs = get_eigenvecs(groundtruth)\n",
    "    eigenvecs = groundtruth.copy()\n",
    "    U = np.matmul(eigenvecs.T, eigenvecs)\n",
    "    P = np.matmul(weights.T, weights)\n",
    "    I = np.eye(weights.shape[1])\n",
    "    \n",
    "    return np.trace(np.matmul(U, I-P))\n",
    "\n",
    "\n",
    "def get_eigenvecs(groundtruth):\n",
    "    \"\"\"\n",
    "    Input: groundtruth in the form of list [1,1,0]\n",
    "    Return: np array, eigenvecs: k by d\n",
    "    \"\"\"\n",
    "    d = len(groundtruth)\n",
    "    eigenvecs = None\n",
    "    for dim in range(d):\n",
    "        if groundtruth[dim] > 0:\n",
    "            eigenvec = np.zeros([1, d])\n",
    "            eigenvec[:, dim] = 1.0\n",
    "            if eigenvecs is None:\n",
    "                eigenvecs = eigenvec\n",
    "            else:\n",
    "                eigenvecs = np.concatenate((eigenvecs, eigenvec), axis=0)\n",
    "        else:\n",
    "            break\n",
    "    return eigenvecs\n",
    "\n",
    "\n",
    "def eval_mse_loss(batch_data, _weights):\n",
    "    \"\"\"\n",
    "    Input: np array, batch_data: n by d, \n",
    "           np array, weights: k by d\n",
    "    \"\"\"\n",
    "    projection = np.matmul(batch_data, _weights.T)\n",
    "    xhat = np.matmul(projection, _weights)\n",
    "    #print(xhat.shape)\n",
    "    return np.sum(np.square(batch_data-xhat))\n",
    "\n",
    "\n",
    "def check_orthonormality(weights):\n",
    "    \"\"\"\n",
    "    TODO: Check that rows of weights are nearly orthonormal\n",
    "    \"\"\"\n",
    "    pass\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initializers\n",
    "def get_random_orthogonal_initializer(k, d, gain=1, seed=None):\n",
    "    tf.reset_default_graph()\n",
    "    init_fn = tf.orthogonal_initializer(gain, seed, dtype=tf.float64)\n",
    "    init_weights = tf.get_variable('init_weights', initializer=init_fn, shape=[k, d])\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())   \n",
    "        _init_weights = sess.run([init_weights])[0]\n",
    "    return _init_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=2\n",
    "d=100\n",
    "gt = [0] * d\n",
    "for i in range(k):\n",
    "    gt[i] = (k - i)**1\n",
    "_init_weights = get_random_orthogonal_initializer(k, d)\n",
    "n_train = 5000\n",
    "eigenvecs = get_eigenvecs(gt)\n",
    "train_data = get_train_data(n_train, groundtruth=gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01802576, -0.0755826 ,  0.09497881,  0.0718243 , -0.01213936,\n",
       "         0.04268317, -0.03475164,  0.07095404,  0.08500038, -0.09164739,\n",
       "        -0.01232038, -0.20132034,  0.11023618, -0.18257222, -0.05522416,\n",
       "         0.00676606, -0.12531486,  0.06493938, -0.09394478,  0.05498912,\n",
       "        -0.06516511,  0.17191228,  0.08446321, -0.08081859,  0.03773436,\n",
       "        -0.01109114,  0.03336719, -0.00092488, -0.1375121 , -0.09040483,\n",
       "         0.14638525, -0.05628244,  0.16483194,  0.0720996 ,  0.00768822,\n",
       "         0.2531904 , -0.18435864, -0.02343086, -0.13592856,  0.02687252,\n",
       "        -0.06696049, -0.09465413,  0.23857647,  0.03392018, -0.23527688,\n",
       "         0.00104344, -0.03582434, -0.00754925, -0.0964212 , -0.16126917,\n",
       "        -0.04503261,  0.18140931,  0.13301326, -0.10250758, -0.04175685,\n",
       "         0.04435037, -0.00876385,  0.06009517,  0.17888442,  0.0494195 ,\n",
       "        -0.02358682,  0.07166876, -0.0070734 ,  0.06492468, -0.11801459,\n",
       "         0.13027541,  0.04540457, -0.00221915, -0.0833476 ,  0.00817635,\n",
       "         0.09444221,  0.01195156, -0.2575644 ,  0.00332488,  0.03599218,\n",
       "        -0.02666707,  0.0283141 , -0.01067731, -0.00636486, -0.096953  ,\n",
       "        -0.06823146, -0.04377101, -0.01353136,  0.0131313 ,  0.1440049 ,\n",
       "         0.08352504, -0.0265729 , -0.02256435,  0.14076737, -0.06471002,\n",
       "        -0.11426663,  0.05505309, -0.07886256, -0.22368744, -0.07553793,\n",
       "        -0.16952503,  0.09490328,  0.00444855, -0.01538143, -0.02694728],\n",
       "       [-0.10147347, -0.02667745, -0.11048151,  0.1840673 , -0.11941202,\n",
       "        -0.00197494,  0.02828269, -0.06276147, -0.18832397,  0.01782783,\n",
       "        -0.0021233 ,  0.10176528, -0.01457007,  0.05754084,  0.09147864,\n",
       "        -0.10494014, -0.09661156, -0.14156264,  0.08940272,  0.05597051,\n",
       "         0.08479468,  0.01913212, -0.12792757, -0.0903966 ,  0.06457999,\n",
       "         0.10214042,  0.09965436,  0.13516964,  0.03242617, -0.23162109,\n",
       "         0.06396955, -0.0996618 ,  0.08447246,  0.03251895,  0.11863796,\n",
       "         0.03354493,  0.05979104,  0.01789725, -0.0578718 , -0.07523246,\n",
       "         0.06087855,  0.04517533,  0.00182294,  0.14538802, -0.12008765,\n",
       "        -0.11784005,  0.08423901, -0.07391783, -0.12378085, -0.12751178,\n",
       "        -0.1588144 , -0.00109159,  0.14005865,  0.14447853, -0.06672591,\n",
       "        -0.07877909,  0.02627584, -0.17725846, -0.21094696,  0.01374234,\n",
       "        -0.05324934, -0.15440625,  0.0018509 , -0.08175648,  0.02624106,\n",
       "        -0.02758413, -0.31233358, -0.00554074,  0.17945658, -0.12246633,\n",
       "         0.06905583,  0.0044749 , -0.14382385, -0.1232015 ,  0.05274242,\n",
       "         0.03730603, -0.09191842,  0.14110866, -0.01204467, -0.08753388,\n",
       "        -0.08464211, -0.05929537, -0.06845459,  0.04891739, -0.00949586,\n",
       "         0.08876491, -0.13471965,  0.0086503 ,  0.07042461,  0.08588304,\n",
       "         0.0212097 ,  0.08826724, -0.13173537,  0.08468245, -0.03449339,\n",
       "         0.09401251, -0.10262029,  0.08974427, -0.05752204,  0.01890797]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_init_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 8*n_train\n",
    "lr = (None, 0.01) \n",
    "#lr = (0.001, 10)\n",
    "algo = KrasulinaPCA(_init_weights, train_data, groundtruth=eigenvecs, learning_rate=lr, max_iter=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial mse: 15313.426589999872\n",
      "The initial loss: 1.982953792437911\n",
      "Finished training 1-th epoch\n",
      "Finished training 2-th epoch\n",
      "Finished training 3-th epoch\n",
      "Finished training 4-th epoch\n",
      "Finished training 5-th epoch\n",
      "Finished training 6-th epoch\n",
      "Finished training 7-th epoch\n"
     ]
    }
   ],
   "source": [
    "algo._train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(algo._train_mse_log[0], algo._train_mse_log[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(algo._groundtruth_eval_log[0], algo._groundtruth_eval_log[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "matplotlib.rcParams['text.latex.unicode'] = True\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "iterations = range(T)\n",
    "#log_iterations = [math.log(t+1) for t in iterations]\n",
    "log_error = [math.log(y) if y > 0 else 0 for y in algo._groundtruth_eval_log]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "gt_eval_epoch_wise = list()\n",
    "epoch_iters = list()\n",
    "for t, e in enumerate(algo._groundtruth_eval_log):\n",
    "    if t % n_train == 0:\n",
    "        epoch += 1\n",
    "        gt_eval_epoch_wise.append(e)\n",
    "        epoch_iters.append(epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(iterations, log_error)\n",
    "plt.plot(iterations, log_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train all variations\n",
    "count = 1\n",
    "n_train = 5000\n",
    "algo_meta_dict = {}\n",
    "for k in [1, 10]:\n",
    "    for d in [10, 100, 500]:\n",
    "        algo_list = []\n",
    "        for eta in [(None, 0.0001/k), (0.001/k, 10)]:\n",
    "            gt = [0] * d\n",
    "            for i in range(k):\n",
    "                gt[i] = (k-i) \n",
    "            _init_weights = get_random_orthogonal_initializer(k, d)\n",
    "            print(f\"Runing the {count}-th experiment\")\n",
    "            count += 1\n",
    "            eigenvecs = get_eigenvecs(gt)\n",
    "            train_data = get_train_data(n_train, groundtruth=gt)\n",
    "            T = 8*n_train\n",
    "            algo_list.append(KrasulinaPCA(_init_weights, train_data, groundtruth=eigenvecs, learning_rate=eta, max_iter=T))\n",
    "            algo_list[-1]._train()\n",
    "            print(\"Initial and final loss\")\n",
    "            print(algo_list[-1]._groundtruth_eval_log[0], algo_list[-1]._groundtruth_eval_log[-1])\n",
    "        key = (k, d)\n",
    "        algo_meta_dict[key] = algo_list\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_meta_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "matplotlib.rcParams['text.latex.unicode'] = True\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, d in algo_meta_dict.keys():\n",
    "    for idx in [0, 1]:\n",
    "        algo_meta_dict[(k, d)][idx]._groundtruth_eval_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot on all\n",
    "label_list = [r\"$\\eta=10^{-4}/k$\", r\"$\\eta^t = \\frac{10^{-3}}{k(t+10)}$\"]\n",
    "\n",
    "def get_log(k, d, idx, algo_dict):\n",
    "    return algo_dict[(k, d)][idx]._groundtruth_eval_log\n",
    "\n",
    "\n",
    "def plotting(k, d):\n",
    "    fig=plt.figure()\n",
    "    for idx in [0, 1]:\n",
    "        log = get_log(k, d, idx, algo_meta_dict)\n",
    "        epoch = 0\n",
    "        gt_eval_epoch_wise = list()\n",
    "        epoch_iters = list()\n",
    "        for t, e in enumerate(log):\n",
    "            if t % n_train == 0:\n",
    "                epoch += 1\n",
    "                gt_eval_epoch_wise.append(e)\n",
    "                epoch_iters.append(epoch)\n",
    "        #\n",
    "        #log_epoch_iters = [math.log(t_e) for t_e in epoch_iters]\n",
    "        log_gt_eval_epoch_wise = [math.log(e) if e > 0 else math.log(e+1) for e in gt_eval_epoch_wise]\n",
    "        if idx == 1:\n",
    "            #lr_params = (0.001/k, 10)\n",
    "            #lr = r\"$\\frac{0.001}{k(t+10)}$\" \n",
    "            plt.plot(epoch_iters, log_gt_eval_epoch_wise, '--', label=label_list[idx])\n",
    "        else:\n",
    "            #lr = r\"\\frac{0.0001}{k}\"\n",
    "            plt.plot(epoch_iters, log_gt_eval_epoch_wise, label=label_list[idx])\n",
    "    plt.legend()\n",
    "    #plt.title(f\"k={k}, d={d}\")\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel(r'log $\\Delta^t$')\n",
    "    fig.savefig(f\"k-{k}-d-{d}.png\")\n",
    "\n",
    "for k, d in algo_meta_dict.keys():\n",
    "    plotting(k,d)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip figs.zip pre/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "iterations = range(T)\n",
    "log_iterations = [math.log(t+1) for t in iterations]\n",
    "log_error = [math.log(y) if y > 0 else 0 for y in algo._groundtruth_eval_log]\n",
    "#const = 100\n",
    "#theo = [-t + const for t in iterations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(iterations, log_error)\n",
    "#plt.plot(iterations, theo)\n",
    "#plt.plot(log_iterations, log_error)\n",
    "#plt.plot(iterations, algo._groundtruth_eval_log)\n",
    "#plt.plot(iterations, algo._train_mse_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_epoch_iters = [math.log(t_e) for t_e in epoch_iters]\n",
    "log_gt_eval_epoch_wise = [math.log(e) for e in gt_eval_epoch_wise]\n",
    "plt.plot(epoch_iters, log_gt_eval_epoch_wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arr = [1,2,3]\n",
    "#print(test_arr[:0])\n",
    "if test_arr[:1]:\n",
    "    print(\"not null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(1000))\n",
    "y = [1/float(t+1) for t in x]\n",
    "y1 = [2**(-t) for t in x]\n",
    "log_y = [math.log(e) for e in y]\n",
    "log_y1 = [math.log(e) for e in y1]\n",
    "plt.plot(x, log_y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/sorki/python-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!brew install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd python-mnist\n",
    "./get_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mndata = MNIST('python-mnist/data')\n",
    "images, labels = mndata.load_training()\n",
    "\n",
    "images = np.array([np.array(data) for data in images[:5000]])\n",
    "\n",
    "data_mean = np.array(images[0])\n",
    "data_c = [np.array(images[0])]\n",
    "\n",
    "for data in images[1:]:\n",
    "    data_mean += np.array(data)\n",
    "    data_c.append(np.array(data))\n",
    "data_mean = data_mean / len(images)\n",
    "\n",
    "data_c = np.array(data_c) - data_mean\n",
    "\n",
    "images = np.array([np.array(data) for data in images[:5000]])\n",
    "\n",
    "data_mean = np.array(images[0])\n",
    "data_c = [np.array(images[0])]\n",
    "\n",
    "for data in images[1:]:\n",
    "    data_mean += np.array(data)\n",
    "    data_c.append(np.array(data))\n",
    "data_mean = data_mean / len(images)\n",
    "\n",
    "data_c = np.array(data_c) - data_mean\n",
    "\n",
    "sigma = 1.0/len(labels) * np.matmul(data_c.T, data_c)\n",
    "\n",
    "np.linalg.matrix_rank(sigma, hermitian=True)\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "pca.fit(sigma)\n",
    "\n",
    "sum(pca.explained_variance_  > 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "k = 185\n",
    "d = 784\n",
    "_init_weights = get_random_orthogonal_initializer(k, d)\n",
    "\n",
    "T = 10*n_train\n",
    "#lr = (None, 0.0001) \n",
    "lr = (0.001, 10)\n",
    "algo = KrasulinaPCA(_init_weights, data_c, groundtruth=pca.components_[:k,:], learning_rate=lr, max_iter=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo._train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(algo._train_mse_log[0], algo._train_mse_log[-1])\n",
    "print(algo._groundtruth_eval_log[0], algo._groundtruth_eval_log[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "gt_eval_epoch_wise = list()\n",
    "epoch_iters = list()\n",
    "for t, e in enumerate(algo._groundtruth_eval_log):\n",
    "    if t % n_train == 0:\n",
    "        epoch += 1\n",
    "        gt_eval_epoch_wise.append(e)\n",
    "        epoch_iters.append(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = range(T)\n",
    "log_iterations = [math.log(t+1) for t in iterations]\n",
    "log_error = [math.log(y) if y > 0 else 0 for y in algo._groundtruth_eval_log]\n",
    "plt.plot(iterations, log_error)\n",
    "#plt.plot(log_iterations, log_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo._groundtruth_eval_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faces low-rank dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import RandomState\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "\n",
    "rng = RandomState(0)\n",
    "\n",
    "# Load faces data\n",
    "dataset = fetch_olivetti_faces(shuffle=True, random_state=rng)\n",
    "faces = dataset.data\n",
    "\n",
    "n_samples, n_features = faces.shape\n",
    "\n",
    "# global centering\n",
    "faces_centered = faces - faces.mean(axis=0)\n",
    "\n",
    "# local centering\n",
    "faces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)\n",
    "\n",
    "print(\"Dataset consists of %d faces\" % n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "pca = decomposition.PCA(n_components=0.8, svd_solver='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(faces_centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "k = 37\n",
    "d = 4096\n",
    "_init_weights = get_random_orthogonal_initializer(k, d)\n",
    "\n",
    "n_train = 400\n",
    "T = 10*n_train\n",
    "#lr = (None, 0.0001) \n",
    "lr = (0.001, 10)\n",
    "algo = KrasulinaPCA(_init_weights, faces_centered, groundtruth=pca.components_, \n",
    "                    learning_rate=lr, mini_batch_size=1, max_iter=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo._train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
