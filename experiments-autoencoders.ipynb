{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ortho_group #generator for random orthogonal matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1.06675339, -0.04445366]),\n",
       " array([-0.48614008, -0.91490363]),\n",
       " array([ 0.61252821,  0.69055214]),\n",
       " array([ 0.72057894, -0.73735813]),\n",
       " array([ 0.792623  ,  0.42707743])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data_generator(dim, noise_bound, gt_dict=None):\n",
    "    if gt_dict is not None:\n",
    "        W = gt_dict\n",
    "    else:\n",
    "        W = ortho_group.rvs(dim)\n",
    "    s = np.random.multinomial(1, [float(1/dim)]*dim)\n",
    "    sigma = float(noise_bound)/(dim)**(0.5)\n",
    "    eps = np.random.normal([0.0]*dim, sigma)\n",
    "    norm = np.linalg.norm(eps)\n",
    "    if norm > noise_bound:\n",
    "        eps = noise_bound * eps/norm\n",
    "    #return np.dot(W,s)+eps, eps\n",
    "    #print(np.dot(W,s))\n",
    "    return np.dot(W,s)+eps\n",
    "    \n",
    "def batch_data_generator(dim, noise_bound, batch_size, gt_dict=None):\n",
    "    def map_function(null):\n",
    "        return data_generator(dim, noise_bound, gt_dict)\n",
    "    \n",
    "    return list(map(map_function, [0]*batch_size))\n",
    "\n",
    "\n",
    "# x, eps = data_generator(2,0.1)\n",
    "# #print(np.linalg.norm(eps))\n",
    "# print(x)\n",
    "\n",
    "list_x = batch_data_generator(2,0.1,5)\n",
    "list_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, -1, 2]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(0, [0,-1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define bias initialization\n",
    "def bias_init(weights, norm, batch_size, batch_data_generator, dim, noise_bound):\n",
    "    assert norm > 1, 'the norm value is invalid!'\n",
    "    ## generate a batch sample\n",
    "    batch_x = batch_data_generator(dim, noise_bound, batch_size)\n",
    "    ## calculate average inner product\n",
    "    avg_proj = tf.reduce_mean(tf.matmul(weights, tf.transpose(np.array(batch_x))), axis=1)\n",
    "    return tf.subtract(tf.divide(avg_proj, norm**2), avg_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## data model parameters\n",
    "dim = 2\n",
    "noise_bound = 0.1\n",
    "\n",
    "## architecture parameters\n",
    "width = 2*dim\n",
    "activation = 'relu'\n",
    "\n",
    "## algorithmic parameters\n",
    "init_batch_size = 200\n",
    "norm = 2\n",
    "rescale_param = 1/float(norm**2) - 1\n",
    "t_o = 100\n",
    "c_prime = 10\n",
    "##\n",
    "train_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gt_dict = ortho_group.rvs(dim) # generate ground-truth dictionary\n",
    "init_weights = tf.random_normal([width, dim], dtype=tf.float64) # init random weights\n",
    "init_bias = bias_init(init_weights, norm, init_batch_size, batch_data_generator, dim, noise_bound)\n",
    "with tf.Session() as sess:\n",
    "    init_weights_ = sess.run(init_weights)\n",
    "    init_bias_ = sess.run(init_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## evaluation metric\n",
    "def cosine_squared_distances(weights, dictionary):\n",
    "    #print(weights.shape)\n",
    "    #print(normalize(weights))\n",
    "    w_normalized = normalize(weights)\n",
    "    #w_normalized = 0.5*weights\n",
    "    raw_scores = np.square(np.matmul(w_normalized, np.transpose(dictionary)))\n",
    "    max_scores = np.max(raw_scores, axis=0) # best approximation for each dict item\n",
    "    return max_scores, raw_scores, min(max_scores)\n",
    "def cos_sq_avg_distances(weights, dictionary):\n",
    "    max_scores,_,_ = cosine_squared_distances(weights, dictionary)\n",
    "    max_scores = [1]*len(max_scores)-max_scores\n",
    "    return np.mean(max_scores)\n",
    "def cos_sq_min_distances(weights, dictionary):\n",
    "    max_scores,_,_ = cosine_squared_distances(weights, dictionary)\n",
    "    max_scores = [1]*len(max_scores)-max_scores\n",
    "    return np.min(max_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.24626159, -0.0881757 ],\n",
       "       [ 0.6835569 , -1.0049918 ],\n",
       "       [-0.93737295, -0.50354038],\n",
       "       [ 0.44191751, -0.56173612]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_weights_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original SGD Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n"
     ]
    }
   ],
   "source": [
    "## Relu - autoencoder model with tied weights\n",
    "\n",
    "### parameters to learn\n",
    "weights = tf.Variable(init_weights_, dtype=tf.float64)\n",
    "#bias = tf.Variable(init_bias_, dtype=tf.float64)\n",
    "bias = tf.Variable(np.zeros(width), dtype=tf.float64)\n",
    "create_row_normalize_op = tf.assign(weights, norm * tf.nn.l2_normalize(weights, dim=1))\n",
    "### define model and loss\n",
    "def encoder(weights, bias, x):\n",
    "    return tf.add(tf.matmul(x, tf.transpose(weights)), bias)\n",
    "def decoder(weights, h, activation):\n",
    "    if activation == 'relu':\n",
    "        return tf.squeeze(tf.matmul(tf.nn.relu(h), weights))\n",
    "    else:\n",
    "        print('activation function not implemented')\n",
    "        exit(0)\n",
    "def data_wise_mean_sq_loss(x, weights, bias):\n",
    "    x_hat = decoder(weights, encoder(weights, bias, x), 'relu')\n",
    "    print(x_hat.get_shape())\n",
    "    return tf.reduce_sum(tf.square(x - x_hat))\n",
    "\n",
    "#x_gt = tf.placeholder(tf.float64, [dim,])\n",
    "mini_batch_size = 100\n",
    "batch_x = tf.placeholder(tf.float64, [mini_batch_size, dim])\n",
    "\n",
    "### define optimizer  \n",
    "global_step = tf.Variable(t_o, trainable=False)\n",
    "learning_rate = c_prime\n",
    "decay_steps = 1\n",
    "decay_rate = 1.0\n",
    "increment_global_step_op = tf.assign(global_step, global_step+1)\n",
    "##\n",
    "learn_rate = tf.train.inverse_time_decay(tf.cast(learning_rate, tf.float64), global_step, decay_steps, decay_rate)\n",
    "#learn_rate = 0.01\n",
    "train_op = tf.train.GradientDescentOptimizer(learn_rate).minimize(data_wise_mean_sq_loss(batch_x, weights, bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## standard learning-rate decayed SGD training\n",
    "\n",
    "#create_row_normalize_op = tf.assign(weights, tf.nn.l2_normalize(weights, dim=1))\n",
    "# eta = 0.1\n",
    "# update_bias_op = tf.assign(bias, \n",
    "#                     bias*(1-eta)+eta*tf.squeeze(tf.matmul(weights, tf.expand_dims(x_gt, axis=1)))*rescale_param)\n",
    "\n",
    "#gt_dict = ortho_group.rvs(dim) # generate ground-truth dictionary\n",
    "#test_sample = batch_data_generator(dim, noise_bound, 100)\n",
    "avg_scores_list = list()\n",
    "min_scores = list()\n",
    "control_weight = False\n",
    "for n_runs in range(10):\n",
    "    avg_scores = list()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(train_steps):\n",
    "            if i==0:\n",
    "                avg_scores.append(cos_sq_avg_distances(init_weights_, gt_dict))\n",
    "            #x = data_generator(dim, noise_bound, gt_dict)[0]\n",
    "            #x = data_generator(dim, noise_bound, gt_dict)\n",
    "            x = batch_data_generator(dim, noise_bound, mini_batch_size, gt_dict)\n",
    "            #init_weights_ = sess.run(init_weights)\n",
    "            _, weights_, bias_ = sess.run([train_op, weights, bias] , feed_dict={batch_x: x})\n",
    "            if control_weight:\n",
    "                _, weights_ = sess.run([create_row_normalize_op, weights]) \n",
    "            n_steps = sess.run(increment_global_step_op)\n",
    "            avg_scores.append(cos_sq_avg_distances(weights_, gt_dict))\n",
    "            min_scores.append(cos_sq_min_distances(weights_, gt_dict))\n",
    "    avg_scores_list.append(avg_scores)\n",
    "            #if (n_steps-t_o) % 100 == 0:\n",
    "#                 print('Training at %d-th iteration'% (n_steps-t_o))\n",
    "#                 print('cosine distances',)\n",
    "#                 print(cosine_squared_distances(weights_, gt_dict)[0])\n",
    "#                 print('Cosine best approximation distance at init', cosine_squared_distances(init_weights_, gt_dict)[0])\n",
    "        #weights_final = weights_.eval()\n",
    "        #bias_final = bias_.eval()\n",
    "#     print('Cosine best approximation distance at init', cosine_squared_distances(init_weights_, gt_dict)[0])\n",
    "#     print('Cosine best approximation after updates', cosine_squared_distances(weights_, gt_dict)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.65766417,  0.99035957])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_squared_distances(init_weights_, gt_dict)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified SGD Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## algorithmic params\n",
    "mini_batch_size = 1\n",
    "b_appx_batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "## Relu - autoencoder model with tied weights\n",
    "\n",
    "### parameters to learn\n",
    "#init_weights = tf.random_normal([width, dim], dtype=tf.float64)\n",
    "weights = tf.Variable(init_weights_, dtype=tf.float64)\n",
    "bias = tf.Variable(init_bias_, trainable=False, dtype=tf.float64)\n",
    "\n",
    "### define model and loss\n",
    "def encoder(weights, bias, x):\n",
    "    return tf.add(tf.matmul(x, tf.transpose(weights)), bias)\n",
    "def decoder(weights, h, activation):\n",
    "    if activation == 'relu':\n",
    "        return tf.squeeze(tf.matmul(tf.nn.relu(h), weights))\n",
    "    else:\n",
    "        print('activation function not implemented')\n",
    "        exit(0)\n",
    "def data_wise_mean_sq_loss(x, weights, bias):\n",
    "    x_hat = decoder(weights, encoder(weights, bias, x), 'relu')\n",
    "    #print(x_hat.get_shape())\n",
    "    return tf.reduce_sum(tf.square(x - x_hat))\n",
    "\n",
    "def batch_wise_mean_sq_loss(batch_x, weights, bias):\n",
    "    batch_x_hat = decoder(weights, encoder(weights, bias, batch_x), 'relu')\n",
    "    print(batch_x_hat.get_shape())\n",
    "    return tf.reduce_mean(tf.square(batch_x - batch_x_hat), axis=1)\n",
    "\n",
    "#x_gt = tf.placeholder(tf.float64, [dim,])\n",
    "batch_x_gt = tf.placeholder(tf.float64, [mini_batch_size, dim])\n",
    "batch_x_update_b = tf.placeholder(tf.float64, [b_appx_batch_size, dim])\n",
    "### define optimizer  \n",
    "global_step = tf.Variable(t_o, trainable=False)\n",
    "learning_rate = c_prime\n",
    "decay_steps = 1\n",
    "decay_rate = 1.0\n",
    "increment_global_step_op = tf.assign(global_step, global_step+1)\n",
    "##\n",
    "learn_rate = tf.train.inverse_time_decay(tf.cast(learning_rate, tf.float64), global_step, decay_steps, decay_rate)\n",
    "#learn_rate = 0.01\n",
    "train_op = tf.train.GradientDescentOptimizer(learn_rate).minimize(batch_wise_mean_sq_loss(batch_x_gt, weights, bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Norm-controlled SGD training\n",
    "\n",
    "create_row_normalize_op = tf.assign(weights, norm * tf.nn.l2_normalize(weights, dim=1))\n",
    "#batch_x = batch_data_generator(dim, noise_bound, b_appx_batch_size)\n",
    "# update_bias_op = tf.assign(bias, \n",
    "#                     tf.squeeze(tf.matmul(weights, tf.expand_dims(x_gt, axis=1)))*rescale_param)\n",
    "\n",
    "def get_bias_update(batch_x, weights, bias, width, batch_size, rescale_param):\n",
    "    if tf.shape(batch_x)==1:\n",
    "        print('wrong')\n",
    "    else:\n",
    "#         relu_activation = tf.nn.relu(tf.transpose(\n",
    "#             tf.add(tf.transpose(tf.matmul(weights, batch_x)),bias)))\n",
    "        #avg_activation = tf.reduce_mean(relu_activation, axis=1)\n",
    "        projection = tf.transpose(tf.matmul(batch_x, tf.transpose(weights)))    # width by batch_size\n",
    "        #print(projection.get_shape())\n",
    "        relu_activation = tf.nn.relu(tf.transpose(tf.add(tf.transpose(projection), bias)))\n",
    "        #print(relu_activation.get_shape())\n",
    "        zero = tf.constant(0, dtype=tf.float64)\n",
    "        where = tf.not_equal(relu_activation, zero) ## logical indexing of relu_activation\n",
    "        indices = tf.where(where) ## indices of nonzero entries in relu_activation\n",
    "        #print('where shape', where.get_shape())\n",
    "        #temp = tf.count_nonzero(where,axis=1)\n",
    "        #print('nonzero', temp.get_shape())\n",
    "        ## calculate empirical prob of firing\n",
    "        nnz = tf.cast(tf.count_nonzero(where, axis=1), tf.float64)\n",
    "        zero_of_nnz = tf.equal(nnz, zero)\n",
    "        offsetted_nnz = tf.where(zero_of_nnz, tf.ones([width], dtype=tf.float64), nnz)\n",
    "        prob_firing = tf.divide(nnz, tf.cast(batch_size,tf.float64))\n",
    "        \n",
    "        ## calculate empirical mean of projected value\n",
    "        shape = tf.constant([width, batch_size], dtype=tf.int64)\n",
    "        #print(indices.get_shape())\n",
    "        updated = tf.scatter_nd(indices, tf.gather_nd(projection, indices), shape)\n",
    "        updated = tf.reduce_sum(updated, axis=1)#\n",
    "        #offsetted_nnz = tf.scatter_add(nnz, indices_of_zero_in_nnz, tf.ones_like(indices_of_zero_in_nnz))\n",
    "        updated = tf.divide(updated, offsetted_nnz) * rescale_param ## shape = (width,)\n",
    "        #print('updated shape', updated.get_shape())\n",
    "        #print(relu_activation.get_shape())\n",
    "        new_bias = tf.add(tf.multiply(prob_firing,updated), tf.multiply(tf.subtract(tf.cast(1,tf.float64),prob_firing),bias))\n",
    "        return new_bias\n",
    "\n",
    "new_bias = get_bias_update(batch_x_update_b, weights, bias, width, b_appx_batch_size, rescale_param)\n",
    "update_bias_op = tf.assign(bias, new_bias)\n",
    "\n",
    "\n",
    "\n",
    "#test_sample = batch_data_generator(dim, noise_bound, 100)\n",
    "\n",
    "min_scores_mod = list()\n",
    "weights_cache = init_weights_\n",
    "#bias_cache = init_bias_\n",
    "avg_scores_mod_list = list()\n",
    "\n",
    "for n_runs in range(10):\n",
    "    avg_scores_mod = list()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(train_steps):\n",
    "            if i == 0:\n",
    "                avg_scores_mod.append(cos_sq_avg_distances(init_weights_, gt_dict))\n",
    "            #x = data_generator(dim, noise_bound, gt_dict)[0]\n",
    "            batch_x = batch_data_generator(dim, noise_bound, mini_batch_size, gt_dict)\n",
    "            #init_weights_ = sess.run(init_weights)\n",
    "            #sess.run([train_op] , feed_dict={x_gt: x})\n",
    "            sess.run([train_op] , feed_dict={batch_x_gt: np.array(batch_x)})\n",
    "            _,weights_ = sess.run([create_row_normalize_op, weights])  ## row-normalization of updated weights\n",
    "\n",
    "            ## get a fresh sample and update bias\n",
    "            #x = data_generator(dim, noise_bound, gt_dict)[0]\n",
    "            #bias_ = sess.run(update_bias_op, feed_dict={x_gt: x})  ## update bias using updated weights\n",
    "            batch_x_for_b = batch_data_generator(dim, noise_bound, b_appx_batch_size, gt_dict)\n",
    "            _, bias_ =sess.run([update_bias_op, bias], \n",
    "                              feed_dict={batch_x_update_b: np.array(batch_x_for_b)})\n",
    "\n",
    "\n",
    "            n_steps = sess.run(increment_global_step_op)\n",
    "            avg_scores_mod.append(cos_sq_avg_distances(weights_, gt_dict))\n",
    "            min_scores_mod.append(cos_sq_min_distances(weights_, gt_dict))\n",
    "    avg_scores_mod_list.append(avg_scores_mod)\n",
    "#         if (n_steps-t_o) % 100 == 0:\n",
    "#             print('Training at %d-th iteration'% (n_steps-t_o))\n",
    "#             print('cosine distances',)\n",
    "#             print(cosine_squared_distances(weights_, gt_dict)[0])\n",
    "#             #print('number of bias entries not updated is %d'%len(ind_))\n",
    "#             print('change of weights', sess.run(tf.norm(weights_ - weights_cache)))\n",
    "#             #print('change of bias', tf.norm(bias_ - bias_cache))\n",
    "#             weights_cache = weights_\n",
    "            #bias_cache = bias_\n",
    "    #weights_final = weights_.eval()\n",
    "    #bias_final = bias_.eval()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('at init', array([ 0.65766417,  0.99035957]))\n",
      "('after updates', array([ 0.9962396 ,  0.99813279]))\n"
     ]
    }
   ],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#best_appx, all_appx = cosine_distances(weights_, gt_dict)\n",
    "#print(best_appx)\n",
    "#print('best of init', cosine_distances(init_weights_, gt_dict)[-1])\n",
    "print('at init', cosine_squared_distances(init_weights_, gt_dict)[0])\n",
    "print('after updates', cosine_squared_distances(weights_, gt_dict)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg1 = np.mean(np.array(avg_scores_list), axis=0)\n",
    "std1 = np.std(np.array(avg_scores_list), axis=0)\n",
    "avg2 = np.mean(np.array(avg_scores_mod_list), axis=0)\n",
    "std2 = np.std(np.array(avg_scores_mod_list), axis=0)\n",
    "len(std2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEOCAYAAACAfcAXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUgIQEYlSSgAiIlFKCUBEJEr9P/bCjwz5TJLQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b254250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "time_pts = range(t_o, t_o+train_steps+1)\n",
    "appx_func = [40/float(t) for t in time_pts]\n",
    "fig, ax = plt.subplots()\n",
    "# ax.plot(time_pts, avg1[:101])\n",
    "# ax.plot(time_pts, avg2[:101])\n",
    "ax.errorbar(time_pts, avg1, yerr=std1, errorevery=200, label='original')\n",
    "ax.errorbar(time_pts, avg2, yerr=std2, errorevery=200, label='modified')\n",
    "# ax.plot(time_pts, avg_scores, label='original')\n",
    "# ax.plot(time_pts, avg_scores_mod, label='modified')\n",
    "ax.plot(time_pts, appx_func, label='theoretical bound')\n",
    "# ax.plot(time_pts, [cos_sq_avg_distances(init_weights_, gt_dict)]*train_steps, label='initial loss')\n",
    "# fig.savefig('original.eps')\n",
    "ax.set_title('squared sin loss vs t')\n",
    "ax.legend(loc=1)\n",
    "fig.savefig('original6' + 'to_' + str(t_o) + '_cprme_' + str(c_prime) + '.eps')\n",
    "ax.loglog()\n",
    "fig.savefig('log6' + 'to_' + str(t_o) + '_cprme_' + str(c_prime) + '.eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
