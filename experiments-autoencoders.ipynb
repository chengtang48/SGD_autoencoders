{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ortho_group #generator for random orthogonal matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import MiniBatchDictionaryLearning\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_generator(dim, noise_bound, dict_type='orthogonal', dict_size=None, gt_dict=None):\n",
    "    if gt_dict is not None:\n",
    "        W = gt_dict\n",
    "    elif dict_type == 'orthogonal':\n",
    "        dict_size = dim\n",
    "        W = ortho_group.rvs(dict_size)\n",
    "    elif dict_type == 'gauss':\n",
    "        if dict_size is None:\n",
    "            dict_size = dim\n",
    "        W = np.random.randn(dict_size, dim)\n",
    "        W_norm = np.linalg.norm(W, axis=1)\n",
    "        W = np.multiply(W.T, W_norm)\n",
    "    #s = np.random.multinomial(1, [1/float(dim)]*dim)\n",
    "    s = np.random.multinomial(1, [1/float(dict_size)]*dict_size)\n",
    "#     sigma = float(noise_bound)/(dim)**(0.5)\n",
    "#     eps = np.random.normal([0.0]*dim, sigma)\n",
    "    sigma = float(noise_bound)/(dict_size)**(0.5)\n",
    "    eps = np.random.normal([0.0]*dim, sigma)\n",
    "    #norm = np.linalg.norm(eps)\n",
    "    #if norm > noise_bound:\n",
    "    #    eps = noise_bound * eps/norm\n",
    "    #return np.dot(W,s)+eps, eps\n",
    "    #print(np.dot(W,s))\n",
    "    return np.sum(np.multiply(W,s), axis=1)+eps\n",
    "    \n",
    "def batch_data_generator(dim, noise_bound, batch_size, dict_type='orthogonal', dict_size=None, gt_dict=None):\n",
    "    def map_function(null):\n",
    "        return list(data_generator(dim, noise_bound, dict_type, dict_size, gt_dict))\n",
    "    \n",
    "    return list(map(map_function, [0]*batch_size))\n",
    "\n",
    "\n",
    "# x = batch_data_generator(dim,noise_bound, 10, dict_type, dict_size, gt_dict)\n",
    "# # #print(np.linalg.norm(eps))\n",
    "# print(len(x[0]))\n",
    "\n",
    "#list_x = batch_data_generator(2,0.1,5)\n",
    "#list_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## data model parameters\n",
    "dim = 64\n",
    "noise_bound = 0.4\n",
    "#dict_size = 128\n",
    "#dict_type = 'gauss'\n",
    "dict_size = 64\n",
    "dict_type = 'orthogonal'\n",
    "#####\n",
    "gt_dict = ortho_group.rvs(dim) # generate ground-truth dictionary\n",
    "# gt_dict = np.random.randn(dict_size, dim)\n",
    "# gt_dict_norm = np.linalg.norm(gt_dict, axis=1)\n",
    "# gt_dict = np.multiply(gt_dict.T, 1./gt_dict_norm)\n",
    "## architecture parameters\n",
    "#width = int(math.log(dim))*dim\n",
    "#width = int(0.6*dim)\n",
    "width = int(0.5*dict_size)\n",
    "activation = 'relu'\n",
    "\n",
    "## algorithmic parameters\n",
    "init_batch_size = 500\n",
    "norm = 2\n",
    "#norm = 0.8\n",
    "#rescale_param = 1/float(norm**2) - 1\n",
    "t_o = 1\n",
    "c_prime = 10\n",
    "##\n",
    "train_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define bias initialization\n",
    "def bias_init(weights, norm, batch_size, batch_data_generator, dim, noise_bound, dict_type, dict_size, gt_dict):\n",
    "    #assert norm > 1, 'the norm value is invalid!'\n",
    "    ## generate a batch sample\n",
    "    batch_x = batch_data_generator(dim, noise_bound, batch_size, dict_type, dict_size, gt_dict)\n",
    "    ## calculate average inner product\n",
    "    avg_proj = tf.reduce_mean(tf.matmul(weights, tf.transpose(np.array(batch_x))), axis=1)\n",
    "    return tf.subtract(tf.divide(avg_proj, norm**2), avg_proj)\n",
    "\n",
    "## evaluation metric\n",
    "def cosine_squared_distances(weights, dictionary):\n",
    "    #print(weights.shape)\n",
    "    #print(normalize(weights))\n",
    "    w_normalized = normalize(weights)\n",
    "    #w_normalized = 0.5*weights\n",
    "    #raw_scores = np.square(np.matmul(w_normalized, np.transpose(dictionary)))\n",
    "    raw_scores = np.square(np.matmul(w_normalized, dictionary))\n",
    "    max_scores = np.max(raw_scores, axis=0) # best approximation for each dict item\n",
    "    #return max_scores, raw_scores, min(max_scores)\n",
    "    return max_scores\n",
    "def cos_sq_avg_distances(weights, dictionary):\n",
    "    #max_scores,_,_ = cosine_squared_distances(weights, dictionary)\n",
    "    max_scores = cosine_squared_distances(weights, dictionary)\n",
    "    max_scores = [1]*len(max_scores)-max_scores\n",
    "    return np.mean(max_scores)\n",
    "def cos_sq_min_distances(weights, dictionary):\n",
    "    #max_scores,_,_ = cosine_squared_distances(weights, dictionary)\n",
    "    max_scores = cosine_squared_distances(weights, dictionary)\n",
    "    max_scores = [1]*len(max_scores)-max_scores\n",
    "    return np.max(max_scores)\n",
    "\n",
    "\n",
    "########\n",
    "#### Experimental aide\n",
    "def get_firing_pattern_op(firing_list, weights, bias, batch_data):\n",
    "    ## get statistics about average firing intensity of each neuron\n",
    "    firing = np.transpose(np.matmul(weights, np.transpose(batch_data)))+bias\n",
    "    firing[firing<=0] = 0\n",
    "    firing[firing>0] = 1\n",
    "    firing = np.mean(firing, axis=0)\n",
    "    firing_list.append(firing)\n",
    "\n",
    "def get_bias_evolution_op(b_list, bias):\n",
    "    ## update b_list to record current bias values\n",
    "    b_list.append(bias)\n",
    "\n",
    "def get_norm_evolution_op(weights_list, weights):\n",
    "    weights_list.append(np.linalg.norm(weights))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.random.randn(100,64).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#init_weights = tf.random_normal([width, dim], dtype=tf.float64) # init random weights\n",
    "#init_weights = tf.constant(batch_data_generator(dim, noise_bound, width, gt_dict=gt_dict), dtype=tf.float64)\n",
    "init_weights = tf.constant(batch_data_generator(dim, noise_bound, width, dict_type, dict_size, gt_dict), \n",
    "                               dtype=tf.float64)\n",
    "init_bias = bias_init(init_weights, norm, init_batch_size, batch_data_generator, \n",
    "                            dim, noise_bound, dict_type, dict_size, gt_dict)\n",
    "with tf.Session() as sess:\n",
    "    init_weights_ = sess.run(init_weights)\n",
    "    #init_weights_ = init_weights\n",
    "    init_bias_ = sess.run(init_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('grounth truth', array([[ -4.35774663e-02,  -1.46271822e-02,  -1.57564989e-02, ...,\n",
      "         -5.33690306e-02,  -2.81685169e-01,  -1.02124913e-01],\n",
      "       [ -8.95427150e-02,  -6.11992207e-02,  -1.07290615e-01, ...,\n",
      "         -2.57374398e-01,  -3.18220446e-01,   1.08167974e-01],\n",
      "       [ -5.40148058e-02,   4.17865133e-02,  -4.54586971e-02, ...,\n",
      "         -2.15172962e-01,  -8.99356831e-02,   1.45284087e-01],\n",
      "       ..., \n",
      "       [  1.25008457e-01,   4.00660987e-02,  -2.06610448e-01, ...,\n",
      "         -7.83654885e-02,   2.16502745e-01,   5.46878572e-02],\n",
      "       [ -1.17554349e-01,  -4.59649567e-03,  -2.23387486e-03, ...,\n",
      "          4.66045596e-02,   1.28108861e-01,   1.60263888e-01],\n",
      "       [ -1.02396302e-01,  -1.88460780e-04,  -8.12887604e-02, ...,\n",
      "         -1.80956409e-01,   4.13581902e-02,  -1.88348250e-01]]))\n",
      "('error of initial weights', 0.65494499469523981)\n"
     ]
    }
   ],
   "source": [
    "#print('learned',init_weights_)\n",
    "print('grounth truth', gt_dict)\n",
    "print('error of initial weights', cos_sq_avg_distances(init_weights_, gt_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online (mini-batch) Dictionary Learning\n",
    "#### See Mairal-Bach-Ponce-Sapiro 09' (implemented in Scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('averaged reconstruction error of ODL is', 0.88599122459323054)\n",
      "('ground truth', array([[ -4.35774663e-02,  -1.46271822e-02,  -1.57564989e-02, ...,\n",
      "         -5.33690306e-02,  -2.81685169e-01,  -1.02124913e-01],\n",
      "       [ -8.95427150e-02,  -6.11992207e-02,  -1.07290615e-01, ...,\n",
      "         -2.57374398e-01,  -3.18220446e-01,   1.08167974e-01],\n",
      "       [ -5.40148058e-02,   4.17865133e-02,  -4.54586971e-02, ...,\n",
      "         -2.15172962e-01,  -8.99356831e-02,   1.45284087e-01],\n",
      "       ..., \n",
      "       [  1.25008457e-01,   4.00660987e-02,  -2.06610448e-01, ...,\n",
      "         -7.83654885e-02,   2.16502745e-01,   5.46878572e-02],\n",
      "       [ -1.17554349e-01,  -4.59649567e-03,  -2.23387486e-03, ...,\n",
      "          4.66045596e-02,   1.28108861e-01,   1.60263888e-01],\n",
      "       [ -1.02396302e-01,  -1.88460780e-04,  -8.12887604e-02, ...,\n",
      "         -1.80956409e-01,   4.13581902e-02,  -1.88348250e-01]]))\n"
     ]
    }
   ],
   "source": [
    "dict_learner = MiniBatchDictionaryLearning(n_components=dict_size, alpha=1, n_iter=1000, fit_algorithm='lars', \n",
    "                                           n_jobs=1, batch_size=1, shuffle=True, dict_init=None, \n",
    "                                           transform_algorithm='omp', transform_n_nonzero_coefs=1, \n",
    "                                           transform_alpha=None, verbose=False, split_sign=False, random_state=None)\n",
    "\n",
    "# get n_components by dim\n",
    "loss = 0\n",
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    dict_learner.fit(batch_data_generator(dim, noise_bound, train_steps, dict_type, dict_size, gt_dict))\n",
    "    loss += cos_sq_avg_distances(dict_learner.components_ , gt_dict)\n",
    "avg_loss_ODL = loss/n_runs\n",
    "print('averaged reconstruction error of ODL is', avg_loss_ODL)\n",
    "#print('learned dictionary', dict_learner.components_)\n",
    "print('ground truth', gt_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original SGD Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Relu - autoencoder model with tied weights\n",
    "\n",
    "### parameters to learn\n",
    "weights = tf.Variable(init_weights_, dtype=tf.float64)\n",
    "#bias = tf.Variable(init_bias_, dtype=tf.float64)\n",
    "bias = tf.Variable(np.zeros(width), dtype=tf.float64, trainable=True)\n",
    "create_row_normalize_op = tf.assign(weights, norm * tf.nn.l2_normalize(weights, dim=1))\n",
    "### define model and loss\n",
    "def encoder(weights, bias, x):\n",
    "    return tf.add(tf.matmul(x, tf.transpose(weights)), bias)\n",
    "def decoder(weights, h, activation):\n",
    "    if activation == 'relu':\n",
    "        return tf.matmul(tf.nn.relu(h), weights)\n",
    "    else:\n",
    "        print('activation function not implemented')\n",
    "        exit(0)\n",
    "\n",
    "###\n",
    "def data_wise_mean_sq_loss(x, weights, bias):\n",
    "    x_hat = decoder(weights, encoder(weights, bias, x), 'relu')\n",
    "    return tf.reduce_mean(tf.reduce_sum(tf.square(x - x_hat), axis=1))\n",
    "\n",
    "#x_gt = tf.placeholder(tf.float64, [dim,])\n",
    "mini_batch_size = 400\n",
    "batch_x = tf.placeholder(tf.float64, [mini_batch_size, dim])\n",
    "\n",
    "### define optimizer  \n",
    "global_step = tf.Variable(t_o, trainable=False)\n",
    "learning_rate = c_prime\n",
    "decay_steps = 1\n",
    "decay_rate = 1.0\n",
    "increment_global_step_op = tf.assign(global_step, global_step+1)\n",
    "##\n",
    "learn_rate = tf.train.inverse_time_decay(tf.cast(learning_rate, tf.float64), global_step, decay_steps, decay_rate)\n",
    "#learn_rate = 0.001\n",
    "init_eval_batch = batch_data_generator(dim, noise_bound, 100, dict_type, dict_size, gt_dict)\n",
    "init_eval_batch = np.array(init_eval_batch, dtype='float64')\n",
    "init_loss = data_wise_mean_sq_loss(init_eval_batch, init_weights_, 0)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    init_loss_ = init_loss.eval()\n",
    "    \n",
    "loss = tf.Variable(init_loss_, trainable=False)\n",
    "update_loss_op = tf.assign(loss, data_wise_mean_sq_loss(batch_x, weights, bias))\n",
    "train_op = tf.train.GradientDescentOptimizer(learn_rate).minimize(data_wise_mean_sq_loss(batch_x, weights, bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## standard learning-rate decayed SGD training\n",
    "\n",
    "#create_row_normalize_op = tf.assign(weights, tf.nn.l2_normalize(weights, dim=1))\n",
    "# eta = 0.1\n",
    "# update_bias_op = tf.assign(bias, \n",
    "#                     bias*(1-eta)+eta*tf.squeeze(tf.matmul(weights, tf.expand_dims(x_gt, axis=1)))*rescale_param)\n",
    "\n",
    "#gt_dict = ortho_group.rvs(dim) # generate ground-truth dictionary\n",
    "#test_sample = batch_data_generator(dim, noise_bound, 100)\n",
    "avg_scores_list = list()\n",
    "losses_list = list()\n",
    "control_weight = False\n",
    "verbose = False\n",
    "for n_runs in range(10):\n",
    "    bias_list = [np.zeros(width)]\n",
    "    #firing_list = list()\n",
    "    avg_scores = list()\n",
    "    losses = [init_loss_]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(train_steps):\n",
    "            x = batch_data_generator(dim, noise_bound, mini_batch_size, dict_type, dict_size, gt_dict)\n",
    "            if i==0:\n",
    "                avg_scores.append(cos_sq_avg_distances(init_weights_, gt_dict))\n",
    "                #get_firing_pattern_op(firing_list, init_weights_, init_bias_, x)    \n",
    "            _, weights_, bias_ = sess.run([train_op, weights, bias], feed_dict={batch_x: x})\n",
    "            if control_weight:\n",
    "                ## normalize weight if control_weight is on\n",
    "                _, weights_ = sess.run([create_row_normalize_op, weights]) \n",
    "            _, loss_ = sess.run([update_loss_op, loss], feed_dict={batch_x: x})\n",
    "            n_steps = sess.run(increment_global_step_op)\n",
    "            #print('reconstruction error', cos_sq_avg_distances(weights_, gt_dict))\n",
    "            avg_scores.append(cos_sq_avg_distances(weights_, gt_dict))\n",
    "            #get_firing_pattern_op(firing_list, weights_, bias_, x)\n",
    "            get_bias_evolution_op(bias_list, bias_)\n",
    "            losses.append(loss_)\n",
    "            if verbose and (n_steps-t_o) % 100 == 0:\n",
    "                print('Training at %d-th iteration'% (n_steps-t_o))\n",
    "                print('Training objective loss', loss_)\n",
    "                print('Dictionary approximation distance', cos_sq_avg_distances(weights_, gt_dict))\n",
    "        avg_scores_list.append(avg_scores)\n",
    "        losses_list.append(losses)\n",
    "        #weights_final = weights_.eval()\n",
    "        #bias_final = bias_.eval()\n",
    "    #print('Cosine best approximation distance at init', cosine_squared_distances(init_weights_, gt_dict)[0])\n",
    "    #print('Cosine best approximation after updates', cosine_squared_distances(weights_, gt_dict)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11b478410>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b4787d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# bias_list\n",
    "time_pts = range(t_o, t_o+train_steps+1)\n",
    "# print(cos_sq_avg_distances(init_weights_, gt_dict))\n",
    "loss_graph = np.mean(np.array(losses_list), axis=0)\n",
    "sine_graph = np.mean(np.array(avg_scores_list), axis=0)\n",
    "#plt.plot(time_pts, bias_list)\n",
    "#len(loss_graph)\n",
    "plt.plot(time_pts, loss_graph)\n",
    "plt.plot(time_pts, sine_graph)\n",
    "# firing_list = np.array(firing_list)\n",
    "# offset = -1\n",
    "# plt.scatter(time_pts[:50], firing_list[offset-50:offset,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified SGD Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## algorithmic params\n",
    "mini_batch_size = 400\n",
    "b_appx_batch_size = 100\n",
    "weights_all = list()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a = np.expand_dims(np.array([1., 1., 1.]), axis=0)\n",
    "# b = 1.0*np.array(range(6)).reshape([3,2])\n",
    "# prod = tf.matmul(a, b)\n",
    "# bias = np.array([1.,-10.])\n",
    "# res1 = tf.nn.relu(tf.add(prod,bias))\n",
    "# res2 = tf.matmul(res1, b.T)\n",
    "# loss = tf.reduce_mean(tf.reduce_sum(tf.square(a-res2), axis=1))\n",
    "# with tf.Session() as sess:\n",
    "#     print(sess.run([prod, res1, res2, loss]))\n",
    "# init_weights_ = weights_all\n",
    "# init_bias = bias_init(init_weights_, 2, 200, batch_data_generator, 2*dim, noise_bound)\n",
    "# with tf.Session() as sess:\n",
    "#     init_bias_ = sess.run(init_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Relu - autoencoder model with tied weights\n",
    "\n",
    "### parameters to learn\n",
    "#init_weights = tf.random_normal([width, dim], dtype=tf.float64)\n",
    "weights = tf.Variable(init_weights_, name='weights_mod', dtype=tf.float64)\n",
    "bias = tf.Variable(init_bias_, trainable=False, dtype=tf.float64)\n",
    "#bias = tf.Variable(np.zeros(width), name = 'bias_mod', trainable=True, dtype=tf.float64)\n",
    "\n",
    "### define model and loss\n",
    "def encoder(weights, bias, x):\n",
    "    return tf.add(tf.matmul(x, tf.transpose(weights)), bias)\n",
    "def decoder(weights, h, activation):\n",
    "    if activation == 'relu':\n",
    "        return tf.matmul(tf.nn.relu(h), weights)\n",
    "    else:\n",
    "        print('activation function not implemented')\n",
    "        exit(0)\n",
    "\n",
    "def data_wise_mean_sq_loss(x, weights, bias):\n",
    "    x_hat = decoder(weights, encoder(weights, bias, x), 'relu')\n",
    "    return tf.reduce_mean(tf.reduce_sum(tf.square(x - x_hat), axis=1))\n",
    "\n",
    "# def batch_wise_mean_sq_loss(batch_x, weights, bias):\n",
    "#     batch_x_hat = decoder(weights, encoder(weights, bias, batch_x), 'relu')\n",
    "#     print(batch_x_hat.get_shape())\n",
    "#     return tf.reduce_mean(tf.square(batch_x - batch_x_hat), axis=1)\n",
    "\n",
    "#x_gt = tf.placeholder(tf.float64, [dim,])\n",
    "batch_x_gt = tf.placeholder(tf.float64, [mini_batch_size, dim])\n",
    "batch_x_update_b = tf.placeholder(tf.float64, [b_appx_batch_size, dim])\n",
    "### define optimizer  \n",
    "global_step = tf.Variable(t_o, trainable=False)\n",
    "rescale_param = tf.Variable(1/float(2**2) - 1, trainable=False, dtype=tf.float64)\n",
    "####\n",
    "learning_rate = c_prime\n",
    "decay_steps = 1\n",
    "decay_rate = 1.0\n",
    "increment_global_step_op = tf.assign(global_step, global_step+1)\n",
    "##\n",
    "learn_rate = tf.train.inverse_time_decay(tf.cast(learning_rate, tf.float64), global_step, decay_steps, decay_rate)\n",
    "#learn_rate = 0.1\n",
    "init_eval_batch = batch_data_generator(dim, noise_bound, 100, dict_type, dict_size, gt_dict)\n",
    "init_eval_batch = np.array(init_eval_batch, dtype='float64')\n",
    "init_loss = data_wise_mean_sq_loss(init_eval_batch, init_weights_, 0)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    init_loss_ = init_loss.eval()\n",
    "    \n",
    "loss = tf.Variable(init_loss_, trainable=False)\n",
    "update_loss_op = tf.assign(loss, data_wise_mean_sq_loss(batch_x_gt, weights, bias))\n",
    "train_op = tf.train.GradientDescentOptimizer(learn_rate).minimize(data_wise_mean_sq_loss(batch_x_gt, weights, bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Norm-controlled SGD training\n",
    "norm_decay_rate = tf.train.inverse_time_decay(tf.cast(1, tf.float64), \n",
    "                                              tf.subtract(global_step, t_o), decay_steps, decay_rate)\n",
    "\n",
    "create_row_normalize_op = tf.assign(weights, (1+norm_decay_rate) * tf.nn.l2_normalize(weights, dim=1))\n",
    "update_rescale_param_op = tf.assign(rescale_param, 1/(1+norm_decay_rate)**2 - 1)\n",
    "# create_row_normalize_op = tf.assign(weights, (0.5+0.3*norm_decay_rate) * tf.nn.l2_normalize(weights, dim=1))\n",
    "# update_rescale_param_op = tf.assign(rescale_param, 1/(0.5+0.3*norm_decay_rate)**2 - 1)\n",
    "#batch_x = batch_data_generator(dim, noise_bound, b_appx_batch_size)\n",
    "# update_bias_op = tf.assign(bias, \n",
    "#                     tf.squeeze(tf.matmul(weights, tf.expand_dims(x_gt, axis=1)))*rescale_param)\n",
    "\n",
    "# def get_bias_update(batch_x, weights, bias, width, batch_size, rescale_param):\n",
    "#     if tf.shape(batch_x)==1:\n",
    "#         print('wrong')\n",
    "#     else:\n",
    "# #         relu_activation = tf.nn.relu(tf.transpose(\n",
    "# #             tf.add(tf.transpose(tf.matmul(weights, batch_x)),bias)))\n",
    "#         #avg_activation = tf.reduce_mean(relu_activation, axis=1)\n",
    "#         projection = tf.transpose(tf.matmul(batch_x, tf.transpose(weights)))    # width by batch_size\n",
    "#         #print(projection.get_shape())\n",
    "#         relu_activation = tf.nn.relu(tf.transpose(tf.add(tf.transpose(projection), bias)))\n",
    "#         #print(relu_activation.get_shape())\n",
    "#         zero = tf.constant(0, dtype=tf.float64)\n",
    "#         where = tf.not_equal(relu_activation, zero) ## logical indexing of relu_activation\n",
    "#         indices = tf.where(where) ## indices of nonzero entries in relu_activation\n",
    "#         #print('where shape', where.get_shape())\n",
    "#         #temp = tf.count_nonzero(where,axis=1)\n",
    "#         #print('nonzero', temp.get_shape())\n",
    "#         ## calculate empirical prob of firing\n",
    "#         nnz = tf.cast(tf.count_nonzero(where, axis=1), tf.float64)\n",
    "#         zero_of_nnz = tf.equal(nnz, zero)\n",
    "#         offsetted_nnz = tf.where(zero_of_nnz, tf.ones([width], dtype=tf.float64), nnz)\n",
    "#         prob_firing = tf.divide(nnz, tf.cast(batch_size,tf.float64))\n",
    "        \n",
    "#         ## calculate empirical mean of projected value\n",
    "#         shape = tf.constant([width, batch_size], dtype=tf.int64)\n",
    "#         #print(indices.get_shape())\n",
    "#         updated = tf.scatter_nd(indices, tf.gather_nd(projection, indices), shape)\n",
    "#         updated = tf.reduce_sum(updated, axis=1)#\n",
    "#         #offsetted_nnz = tf.scatter_add(nnz, indices_of_zero_in_nnz, tf.ones_like(indices_of_zero_in_nnz))\n",
    "#         updated = tf.divide(updated, offsetted_nnz) * rescale_param ## shape = (width,)\n",
    "#         #print('updated shape', updated.get_shape())\n",
    "#         #print(relu_activation.get_shape())\n",
    "#         new_bias = tf.add(tf.multiply(prob_firing,updated), tf.multiply(tf.subtract(tf.cast(1,tf.float64),prob_firing),bias))\n",
    "#         return new_bias\n",
    "\n",
    "# def get_bias_update(weights_new, weights, bias, rescale_param, batch_x):\n",
    "#     batch_x = np.array(batch_x)\n",
    "#     hidden = np.transpose(np.add(np.transpose(np.matmul(weights, batch_x.T)), bias))\n",
    "#     hidden[hidden>0] = 1\n",
    "#     hidden[hidden<=0] = 0\n",
    "#     projection = np.matmul(weights_new, batch_x.T)\n",
    "#     bias_new = np.mean(np.multiply(projection, hidden), axis=1)\n",
    "#     return bias_new*rescale_param\n",
    "\n",
    "def get_bias_update(weights_new, bias, rescale_param, batch_x):\n",
    "    batch_x = np.array(batch_x)\n",
    "    hidden = np.transpose(np.add(np.transpose(np.matmul(weights_new, batch_x.T)), bias))\n",
    "    hidden[hidden>0] = 1\n",
    "    hidden[hidden<=0] = 0\n",
    "    projection = np.matmul(weights_new, batch_x.T)\n",
    "    bias_new = np.mean(np.multiply(projection, hidden), axis=1)\n",
    "    return bias_new*rescale_param\n",
    "\n",
    "def reinitialize_weights(weights, bias, batch_x, gt_dict, dim, noise_bound, dict_type, dict_size):\n",
    "    batch_x = np.array(batch_x)\n",
    "    hidden = np.transpose(np.add(np.transpose(np.matmul(weights, batch_x.T)), bias))\n",
    "    hidden[hidden>0] = 1\n",
    "    hidden[hidden<=0] = 0\n",
    "    ####\n",
    "    firing_prob = np.mean(hidden, axis=1)\n",
    "    ind = firing_prob <= 0.8\n",
    "    data_raw = batch_data_generator(dim, noise_bound, np.sum(ind), dict_type, dict_size, gt_dict)\n",
    "    data_norms = np.linalg.norm(data_raw, axis=1)\n",
    "    ## normalize data\n",
    "    weights[ind] = np.transpose(np.multiply(np.transpose(data_raw), data_norms))\n",
    "    return weights\n",
    "    \n",
    "\n",
    "#new_bias = get_bias_update(batch_x_update_b, weights, bias, width, b_appx_batch_size, rescale_param)\n",
    "#update_bias_op = tf.assign(bias, new_bias)\n",
    "### Define some handy operations to update network variables\n",
    "bias_new = tf.placeholder(tf.float64, [width])\n",
    "update_bias_op = tf.assign(bias, bias_new)\n",
    "###\n",
    "weights_new = tf.placeholder(tf.float64, [width, dim])\n",
    "update_weights_op = tf.assign(weights, weights_new)\n",
    "\n",
    "\n",
    "\n",
    "#test_sample = batch_data_generator(dim, noise_bound, 100)\n",
    "\n",
    "#min_scores_mod = list()\n",
    "#weights_cache = init_weights_\n",
    "#bias_cache = init_bias_\n",
    "avg_scores_mod_list = list()\n",
    "losses_mod_list = list()\n",
    "control_norm = True\n",
    "control_bias = True\n",
    "reassign_weights = False\n",
    "##### Training\n",
    "for n_runs in range(10):\n",
    "    avg_scores_mod = list()\n",
    "    losses_mod = [init_loss_]\n",
    "    #firing_list_mod = list()\n",
    "    bias_list_mod = [init_bias_]\n",
    "    weights_cached = init_weights_\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(train_steps):\n",
    "            if i == 0:\n",
    "                avg_scores_mod.append(cos_sq_avg_distances(init_weights_, gt_dict))\n",
    "            #x = data_generator(dim, noise_bound, gt_dict)[0]\n",
    "            batch_x = batch_data_generator(dim, noise_bound, mini_batch_size, dict_type, dict_size, gt_dict)\n",
    "            #init_weights_ = sess.run(init_weights)\n",
    "            #sess.run([train_op] , feed_dict={x_gt: x})\n",
    "            _, weights_, bias_ = sess.run([train_op, weights, bias] , feed_dict={batch_x_gt: batch_x})\n",
    "            \n",
    "            if reassign_weights:\n",
    "                weights_ = reinitialize_weights(weights_, bias_, batch_x, gt_dict, dim, \n",
    "                                                     noise_bound, dict_type, dict_size)\n",
    "                _, weights_ = sess.run([update_weights_op, weights], feed_dict={weights_new: weights_})\n",
    "            \n",
    "            if control_norm:\n",
    "                _,weights_ = sess.run([create_row_normalize_op, weights])  ## row-normalization of updated weights\n",
    "            \n",
    "            if control_bias:\n",
    "                batch_x_new = batch_data_generator(dim, noise_bound, b_appx_batch_size, dict_type, dict_size, gt_dict)\n",
    "                _, rescale_param_ = sess.run([update_rescale_param_op, rescale_param])\n",
    "                #bias_ = get_bias_update(weights_, weights_cached, bias_, rescale_param_, batch_x_new)\n",
    "                bias_ = get_bias_update(weights_, bias_, rescale_param_, batch_x_new)\n",
    "                _,_ = sess.run([update_bias_op, bias], feed_dict={bias_new: bias_})\n",
    "                weights_cached = weights_\n",
    "            \n",
    "            _, loss_ = sess.run([update_loss_op, loss], feed_dict={batch_x_gt: batch_x})\n",
    "            ## get a fresh sample and update bias\n",
    "            #x = data_generator(dim, noise_bound, gt_dict)[0]\n",
    "            #bias_ = sess.run(update_bias_op, feed_dict={x_gt: x})  ## update bias using updated weights\n",
    "            #batch_x_for_b = batch_data_generator(dim, noise_bound, b_appx_batch_size, gt_dict)\n",
    "            #_, bias_ = sess.run([update_bias_op, bias],feed_dict={batch_x_update_b: np.array(batch_x_for_b)})\n",
    "            #print('bias norm', np.sum(np.square(bias_))**0.5)\n",
    "\n",
    "            n_steps = sess.run(increment_global_step_op)\n",
    "            avg_scores_mod.append(cos_sq_avg_distances(weights_, gt_dict))\n",
    "            losses_mod.append(loss_)\n",
    "            #min_scores_mod.append(cos_sq_min_distances(weights_, gt_dict))\n",
    "            get_bias_evolution_op(bias_list_mod, bias_)\n",
    "            #get_firing_pattern_op(firing_list_mod, weights_, bias_, batch_x)\n",
    "    avg_scores_mod_list.append(avg_scores_mod)\n",
    "    losses_mod_list.append(losses_mod)\n",
    "#         if (n_steps-t_o) % 100 == 0:\n",
    "#             print('Training at %d-th iteration'% (n_steps-t_o))\n",
    "#             print('cosine distances',)\n",
    "#             print(cosine_squared_distances(weights_, gt_dict)[0])\n",
    "#             #print('number of bias entries not updated is %d'%len(ind_))\n",
    "#             print('change of weights', sess.run(tf.norm(weights_ - weights_cache)))\n",
    "#             #print('change of bias', tf.norm(bias_ - bias_cache))\n",
    "#             weights_cache = weights_\n",
    "            #bias_cache = bias_\n",
    "    #weights_final = weights_.eval()\n",
    "    #bias_final = bias_.eval()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def get_bias_update(weights_new, weights, bias, rescale_param, batch_x):\n",
    "#     batch_x = np.array(batch_x)\n",
    "#     hidden = np.transpose(np.add(np.transpose(np.matmul(weights, batch_x.T)), bias))\n",
    "#     hidden[hidden>0] = 1\n",
    "#     hidden[hidden<=0] = 0\n",
    "#     projection = np.matmul(weights_new, batch_x.T)\n",
    "#     res = np.multiply(projection, hidden)\n",
    "#     return np.mean(res, axis=1)\n",
    "\n",
    "# weights_new = np.array(range(6)).reshape([3,2])\n",
    "# weights = np.array(range(1,7)).reshape([3,2])\n",
    "# bias = np.array([range(3)])\n",
    "# rescale_param = -0.5\n",
    "# batch_x = np.array(range(20)).reshape([10,2])\n",
    "# print(get_bias_update(weights_new, weights, bias, rescale_param, batch_x))\n",
    "#tf.all_variables()\n",
    "time_pts = range(t_o, t_o+train_steps+1)\n",
    "loss_graph = np.mean(np.array(losses_mod_list), axis=0)\n",
    "sine_graph = np.mean(np.array(avg_scores_mod_list), axis=0)\n",
    "plt.plot(time_pts, loss_graph)\n",
    "plt.plot(time_pts, sine_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#best_appx, all_appx = cosine_distances(weights_, gt_dict)\n",
    "#print(best_appx)\n",
    "#print('best of init', cosine_distances(init_weights_, gt_dict)[-1])\n",
    "print('at init', cosine_squared_distances(init_weights_, gt_dict))\n",
    "print('after updates', cosine_squared_distances(weights_, gt_dict))\n",
    "print('final loss', cos_sq_avg_distances(weights_, gt_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.mean(firing_list_mod, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg1 = np.mean(np.array(avg_scores_list), axis=0)\n",
    "std1 = np.std(np.array(avg_scores_list), axis=0)\n",
    "avg2 = np.mean(np.array(avg_scores_mod_list), axis=0)\n",
    "std2 = np.std(np.array(avg_scores_mod_list), axis=0)\n",
    "#len(std2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(avg_loss_ODL, avg1[-1], avg2[-1])\n",
    "#print(avg2[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_pts = range(t_o, t_o+train_steps+1)\n",
    "appx_func = [10/float(t) for t in time_pts]\n",
    "fig, ax = plt.subplots()\n",
    "# ax.plot(time_pts, avg1[:101])\n",
    "# ax.plot(time_pts, avg2[:101])\n",
    "ax.errorbar(time_pts, avg1, yerr=std1, errorevery=200, label='original')\n",
    "ax.errorbar(time_pts, avg2, yerr=std2, errorevery=200, label='norm-bias controlled')\n",
    "#ax.plot(time_pts, avg_scores_list[0], label='original')\n",
    "#ax.plot(time_pts, avg_scores_mod_list[-1], label='modified')\n",
    "ax.plot(time_pts, appx_func, label='theoretical bound')\n",
    "# ax.plot(time_pts, [cos_sq_avg_distances(init_weights_, gt_dict)]*train_steps, label='initial loss')\n",
    "# fig.savefig('original.eps')\n",
    "ax.set_title('squared sin loss vs t')\n",
    "ax.legend(loc=4)\n",
    "#fig.savefig('original6' + 'to_' + str(t_o) + '_cprme_' + str(c_prime) + '.eps')\n",
    "#ax.loglog()\n",
    "#fig.savefig('log6' + 'to_' + str(t_o) + '_cprme_' + str(c_prime) + '.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(cosine_squared_distances(weights_, gt_dict))\n",
    "weights_all.append(weights_)\n",
    "# weights_all = np.append(weights_all[0], weights_all[1], axis=0)\n",
    "# print(cos_sq_avg_distances(weights_all, gt_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(time_pts, np.array(bias_list_mod)[:,0])\n",
    "#bias_list_mod\n",
    "#np.histogram(bias_list_mod[:,0])\n",
    "#len(time_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from exp import maybe_pickle\n",
    "# import math\n",
    "\n",
    "# def get_data_and_plot(fnames, pname, pvalues, varname, value_list, algo):\n",
    "#     fig, ax = plt.subplots()\n",
    "#     for fname in fnames:\n",
    "#         #print(fname)\n",
    "#         evaluations_over_var = maybe_pickle(fname)\n",
    "#         #fname = 'norm_controlled_data_dim_32_data_dim_2_4_8_16_32width4'\n",
    "#         #evaluations_over_var = maybe_pickle(fname)\n",
    "#         #print(evaluations_over_var)\n",
    "#         #y_array2, y_err_array2 = maybe_pickle(fname2)\n",
    "#     #     cprime, t_o = algo.eta_params\n",
    "#     #     x = range(t_o, t_o+train_steps+1)\n",
    "#     #     appx_func = [40/float(t) for t in x]\n",
    "#         y_array, y_array_err = zip(*evaluations_over_var)\n",
    "#         #print(np.array(y_array))\n",
    "#         #fig, ax = plt.subplots()\n",
    "#         y = np.array(y_array)[:,-1]\n",
    "#         y_err = np.array(y_array_err)[:,-1]\n",
    "#         if varname == 'learn_rate':\n",
    "#             myxticks = value_list\n",
    "#             print(myxticks)\n",
    "#             x = range(len(myxticks))\n",
    "#             ax.set_xticks(x)\n",
    "#             ax.set_xticklabels(myxticks)\n",
    "#             ax.scatter(x, y)\n",
    "#         else:\n",
    "#     #         myxticks = value_list\n",
    "#     #         print(myxticks)\n",
    "#     #         #x = range(len(myxticks))\n",
    "#     #         xlist = map(float,value_list)\n",
    "#     #         ax.set_xticks(xlist)\n",
    "#     #         ax.set_xticklabels(myxticks)\n",
    "#             #ax.scatter(value_list, y)\n",
    "#             #ax.plot([0],y[0],'r*')\n",
    "#             start_idx = fname.index('w')\n",
    "#             ax.plot(value_list, y, '--o', label=fname[start_idx:])\n",
    "#             #ax.plot(value_list,y2, '--ro', label = 'original')\n",
    "#             ax.legend(loc=4)\n",
    "#         #ax.set_xlim(xmin=-2)\n",
    "#     # for idx, y in enumerate(y_array.tolist()):\n",
    "#     #     string = ('%s = %s' %(varname, str(value_list[idx])))\n",
    "#     #     y_err = y_err_array[idx]\n",
    "#     #     ax.errorbar(x, y, yerr=y_err, errorevery=200, label=string)\n",
    "#     # ax.plot(time_pts, avg1[:101])\n",
    "#     # ax.plot(time_pts, avg2[:101])\n",
    "#     #ax.errorbar(x, y, yerr=yerr, errorevery=200, label=arguments['<algo>'])\n",
    "#     #ax.plot(x, appx_func, label='theoretical bound')\n",
    "#     #ax.set_title('squared sin loss vs %s' %varname)\n",
    "#     ax.set_title('loss versus dimension with different network widths')\n",
    "#     ax.set_xlabel('data dimension')\n",
    "#     ax.set_ylabel('squared sine error')\n",
    "# #     pvalues_list = pvalues.split(',')\n",
    "# #     pvalues = '_'.join(pvalues_list)\n",
    "#     if varname == 'learn_rate':\n",
    "#         value_list = ['-'.join(v.split(',')) for v in value_list]\n",
    "#     vvalues = '_'.join(value_list)\n",
    "#     #plt.show()\n",
    "#     #fname = algo +'_'+ pname +'_'+pvalues+'_'+ varname +'_' + vvalues\n",
    "#     fname = algo +'_'+ varname +'_' + vvalues\n",
    "#     fig.savefig(fname+'.eps')\n",
    "#     #_ = maybe_pickle(fname, data=[y_array, y_err_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fnames = list()\n",
    "# fname = 'norm_controlled_data_dim_32_data_dim_2_4_8_16_32width'\n",
    "# for i in range(4):\n",
    "#     fnames.append(fname + str(2**(i+2)))\n",
    "# #fname2 = 'norm_controlled_data_dim_32_data_dim_2_4_8_16_32width8'\n",
    "# #pname = 'batch_size'\n",
    "# #pvalues = '1,200-0-100'\n",
    "# varname = 'data_dim'\n",
    "# value_list = map(str, [4,8,16,32,64])\n",
    "# algo = 'norm_controlled'\n",
    "# get_data_and_plot(fnames, 0, 0, varname, value_list, algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
