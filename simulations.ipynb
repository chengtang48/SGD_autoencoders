{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan for numerical simulations\n",
    "\n",
    "- low-rank data generation: high dimensional, from a subset of coordinates (these are eigenvectors)\n",
    "- krasulina implementation (done)\n",
    "- TODO:\n",
    "    - investigate robustness of the algorithm to eigenvalue perturbations\n",
    "    - investigate the effect of dimension and rank\n",
    "    - investigate perturbed version (optional)\n",
    "    - compare to Ohad Shamir's SVRG variant of Oja's method (optional)\n",
    "- Simulation on real data (candidates: MNIST, CCAT?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tcheng/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/Users/tcheng/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from krasulina import KrasulinaPCA, get_random_orthogonal_initializer, get_eigenvecs\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi']= 300\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "matplotlib.rcParams['text.latex.unicode'] = True\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generators: data should be centered and dispersed around the subspace spanned by gt\n",
    "\n",
    "def get_train_data(n_train, k, groundtruth=None, perturb=0):\n",
    "    \"\"\"\n",
    "    gt: d-dimensional vec (non-increasing entries): \n",
    "    nonzero at coordinates corresponding to eigenvectors, and the value corresponds eigenvalue\n",
    "    example: [1,1,0,0,0]\n",
    "    return: np array train data\n",
    "    \"\"\"\n",
    "    if not groundtruth:\n",
    "        print(\"The data generator without groundtruth is not implemented!\")\n",
    "        return\n",
    "    else:\n",
    "        train_data = None\n",
    "        d = len(groundtruth)\n",
    "        for _ in range(n_train):\n",
    "            x = np.zeros([1, d])\n",
    "            for i in range(d):\n",
    "                if groundtruth[i] > 0:\n",
    "                    x[:, i] = np.random.normal(0, groundtruth[i]**0.5)\n",
    "                elif perturb != 0:\n",
    "                    v = k*perturb / (d-k)\n",
    "                    x[:, i] = np.random.normal(0, v**0.5)\n",
    "            if train_data is None:\n",
    "                train_data = x\n",
    "            else:\n",
    "                train_data = np.concatenate((train_data, x), axis=0)  \n",
    "            #print(train_data.shape)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(train_data, groundtruth):\n",
    "    eigenvecs = get_eigenvecs(groundtruth)\n",
    "    # get random rotation matrix\n",
    "    return data_rot, eigenvecs_rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## try different combinations of k, d\n",
    "## k=1, 10, 50; d=100, 500, 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for each fixed d, k, vary perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1\n",
    "d=100\n",
    "gt = [0] * d\n",
    "for i in range(k):\n",
    "    gt[i] = 1\n",
    "_init_weights = get_random_orthogonal_initializer(k, d)\n",
    "n_train = 5000\n",
    "eigenvecs = get_eigenvecs(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0658195 , -0.09121887, -0.04498019, ...,  0.10632771,\n",
       "        -0.08943787,  0.01871137],\n",
       "       [ 0.00145304, -0.06217551,  0.06160185, ...,  0.1664262 ,\n",
       "         0.07803559, -0.03758566],\n",
       "       [ 0.00750704, -0.00639548,  0.08628903, ..., -0.06085964,\n",
       "         0.03680186, -0.08055913],\n",
       "       ...,\n",
       "       [-0.0190741 , -0.02851426,  0.03917131, ..., -0.03639327,\n",
       "        -0.09142549,  0.00460356],\n",
       "       [ 0.02728887,  0.03779824, -0.03372117, ..., -0.03410528,\n",
       "         0.02773056,  0.02946563],\n",
       "       [-0.01104131,  0.02092904, -0.05261696, ...,  0.05988045,\n",
       "        -0.01123467,  0.03647983]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_init_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturb_ratio = [0, 0.01, 0.1, 0.5]\n",
    "train_data = get_train_data(n_train, k, groundtruth=gt, perturb=gt[0]*perturb_ratio[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \\tau: 0.01 or smaller for d=100 ?\n",
    "\n",
    "T = 6\n",
    "lr = (None, 0.0005) \n",
    "ratio = 0.01\n",
    "algo = KrasulinaPCA(_init_weights, train_data, groundtruth=eigenvecs, \n",
    "                    learning_rate=lr, max_iter=T * n_train, log_freq=ratio*n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial mse: 339445.53252950066\n",
      "The initial loss: 45.31370257586241\n",
      "The loss at the 0-th epoch 50-th iteration is 45.13238847249647\n",
      "The loss at the 0-th epoch 100-th iteration is 44.944015644422706\n",
      "The loss at the 0-th epoch 150-th iteration is 44.76819822018146\n",
      "The loss at the 0-th epoch 200-th iteration is 44.55292725454953\n",
      "The loss at the 0-th epoch 250-th iteration is 44.34292520745758\n",
      "The loss at the 0-th epoch 300-th iteration is 44.11402234440395\n",
      "The loss at the 0-th epoch 350-th iteration is 43.88453746983617\n",
      "The loss at the 0-th epoch 400-th iteration is 43.66040647603295\n",
      "The loss at the 0-th epoch 450-th iteration is 43.42493525466362\n",
      "The loss at the 0-th epoch 500-th iteration is 43.18487713041223\n",
      "The loss at the 0-th epoch 550-th iteration is 42.940367618676405\n",
      "The loss at the 0-th epoch 600-th iteration is 42.68851263178032\n",
      "The loss at the 0-th epoch 650-th iteration is 42.43402997614952\n",
      "The loss at the 0-th epoch 700-th iteration is 42.155742773353126\n",
      "The loss at the 0-th epoch 750-th iteration is 41.87121824363983\n",
      "The loss at the 0-th epoch 800-th iteration is 41.58815040748489\n",
      "The loss at the 0-th epoch 850-th iteration is 41.299387793264984\n",
      "The loss at the 0-th epoch 900-th iteration is 41.02572889512997\n",
      "The loss at the 0-th epoch 950-th iteration is 40.730996206917304\n",
      "The loss at the 0-th epoch 1000-th iteration is 40.4336208252394\n",
      "The loss at the 0-th epoch 1050-th iteration is 40.13016004935553\n",
      "The loss at the 0-th epoch 1100-th iteration is 39.82136706849373\n",
      "The loss at the 0-th epoch 1150-th iteration is 39.5082977924408\n",
      "The loss at the 0-th epoch 1200-th iteration is 39.19135561391865\n",
      "The loss at the 0-th epoch 1250-th iteration is 38.867523929000214\n",
      "The loss at the 0-th epoch 1300-th iteration is 38.53224910887218\n",
      "The loss at the 0-th epoch 1350-th iteration is 38.20688066805151\n",
      "The loss at the 0-th epoch 1400-th iteration is 37.878978133905775\n",
      "The loss at the 0-th epoch 1450-th iteration is 37.51455104043528\n",
      "The loss at the 0-th epoch 1500-th iteration is 37.15023202943154\n",
      "The loss at the 0-th epoch 1550-th iteration is 36.77930299220543\n",
      "The loss at the 0-th epoch 1600-th iteration is 36.43146569200284\n",
      "The loss at the 0-th epoch 1650-th iteration is 36.05996987766767\n",
      "The loss at the 0-th epoch 1700-th iteration is 35.69042545972714\n",
      "The loss at the 0-th epoch 1750-th iteration is 35.31217798884401\n",
      "The loss at the 0-th epoch 1800-th iteration is 34.919083899678\n",
      "The loss at the 0-th epoch 1850-th iteration is 34.54549904670738\n",
      "The loss at the 0-th epoch 1900-th iteration is 34.15666931150717\n",
      "The loss at the 0-th epoch 1950-th iteration is 33.768433688830875\n",
      "The loss at the 0-th epoch 2000-th iteration is 33.39736011942973\n",
      "The loss at the 0-th epoch 2050-th iteration is 32.99596144463693\n",
      "The loss at the 0-th epoch 2100-th iteration is 32.643565987609165\n",
      "The loss at the 0-th epoch 2150-th iteration is 32.26520029252027\n",
      "The loss at the 0-th epoch 2200-th iteration is 31.892554931819927\n",
      "The loss at the 0-th epoch 2250-th iteration is 31.48978693661408\n",
      "The loss at the 0-th epoch 2300-th iteration is 31.099633930595882\n",
      "The loss at the 0-th epoch 2350-th iteration is 30.70676552000137\n",
      "The loss at the 0-th epoch 2400-th iteration is 30.320373792045803\n",
      "The loss at the 0-th epoch 2450-th iteration is 29.920592482341434\n",
      "The loss at the 0-th epoch 2500-th iteration is 29.515143524786197\n",
      "The loss at the 0-th epoch 2550-th iteration is 29.105802938410626\n",
      "The loss at the 0-th epoch 2600-th iteration is 28.705319449825435\n",
      "The loss at the 0-th epoch 2650-th iteration is 28.331871275894557\n",
      "The loss at the 0-th epoch 2700-th iteration is 27.955238099292536\n",
      "The loss at the 0-th epoch 2750-th iteration is 27.555055326243377\n",
      "The loss at the 0-th epoch 2800-th iteration is 27.170039231231712\n",
      "The loss at the 0-th epoch 2850-th iteration is 26.77973982591105\n",
      "The loss at the 0-th epoch 2900-th iteration is 26.398122155456218\n",
      "The loss at the 0-th epoch 2950-th iteration is 25.9896652141056\n",
      "The loss at the 0-th epoch 3000-th iteration is 25.61495225967996\n",
      "The loss at the 0-th epoch 3050-th iteration is 25.247443236128895\n",
      "The loss at the 0-th epoch 3100-th iteration is 24.866362145558128\n",
      "The loss at the 0-th epoch 3150-th iteration is 24.475941653921364\n",
      "The loss at the 0-th epoch 3200-th iteration is 24.119882359496284\n",
      "The loss at the 0-th epoch 3250-th iteration is 23.739726226565054\n",
      "The loss at the 0-th epoch 3300-th iteration is 23.37501712920872\n",
      "The loss at the 0-th epoch 3350-th iteration is 23.0258030586297\n",
      "The loss at the 0-th epoch 3400-th iteration is 22.669353640972005\n",
      "The loss at the 0-th epoch 3450-th iteration is 22.304466226365758\n",
      "The loss at the 0-th epoch 3500-th iteration is 21.945805894443193\n",
      "The loss at the 0-th epoch 3550-th iteration is 21.60788940083617\n",
      "The loss at the 0-th epoch 3600-th iteration is 21.27589564469514\n",
      "The loss at the 0-th epoch 3650-th iteration is 20.927448104371468\n",
      "The loss at the 0-th epoch 3700-th iteration is 20.583444877696074\n",
      "The loss at the 0-th epoch 3750-th iteration is 20.247175166810827\n",
      "The loss at the 0-th epoch 3800-th iteration is 19.901930771615582\n",
      "The loss at the 0-th epoch 3850-th iteration is 19.565814862602075\n",
      "The loss at the 0-th epoch 3900-th iteration is 19.223862011621428\n",
      "The loss at the 0-th epoch 3950-th iteration is 18.891300595984873\n",
      "The loss at the 0-th epoch 4000-th iteration is 18.568108366731856\n",
      "The loss at the 0-th epoch 4050-th iteration is 18.229462832414185\n",
      "The loss at the 0-th epoch 4100-th iteration is 17.90059241948934\n",
      "The loss at the 0-th epoch 4150-th iteration is 17.58008247025528\n",
      "The loss at the 0-th epoch 4200-th iteration is 17.249570320815756\n",
      "The loss at the 0-th epoch 4250-th iteration is 16.954907923311936\n",
      "The loss at the 0-th epoch 4300-th iteration is 16.667574256928535\n",
      "The loss at the 0-th epoch 4350-th iteration is 16.360586334818933\n",
      "The loss at the 0-th epoch 4400-th iteration is 16.075112221212954\n",
      "The loss at the 0-th epoch 4450-th iteration is 15.793211187134961\n",
      "The loss at the 0-th epoch 4500-th iteration is 15.49849922318252\n",
      "The loss at the 0-th epoch 4550-th iteration is 15.237224849570076\n",
      "The loss at the 0-th epoch 4600-th iteration is 14.958191311941402\n",
      "The loss at the 0-th epoch 4650-th iteration is 14.692333491442607\n",
      "The loss at the 0-th epoch 4700-th iteration is 14.415435950883754\n",
      "The loss at the 0-th epoch 4750-th iteration is 14.147103811208588\n",
      "The loss at the 0-th epoch 4800-th iteration is 13.880574566174488\n",
      "The loss at the 0-th epoch 4850-th iteration is 13.622905877094766\n",
      "The loss at the 0-th epoch 4900-th iteration is 13.368663040735786\n",
      "The loss at the 0-th epoch 4950-th iteration is 13.113018056801778\n",
      "Finished training 1-th epoch with total 5000 iterations\n",
      "The current learning rate is 0.0005\n",
      "The loss at the 1-th epoch 5000-th iteration is 12.85953373951242\n",
      "The loss at the 1-th epoch 5050-th iteration is 12.626570287895767\n",
      "The loss at the 1-th epoch 5100-th iteration is 12.386946118822856\n",
      "The loss at the 1-th epoch 5150-th iteration is 12.16596419400966\n",
      "The loss at the 1-th epoch 5200-th iteration is 11.941432011663858\n",
      "The loss at the 1-th epoch 5250-th iteration is 11.731944078776447\n",
      "The loss at the 1-th epoch 5300-th iteration is 11.507160157164613\n",
      "The loss at the 1-th epoch 5350-th iteration is 11.311686870642767\n",
      "The loss at the 1-th epoch 5400-th iteration is 11.092200806329576\n",
      "The loss at the 1-th epoch 5450-th iteration is 10.878048491832807\n",
      "The loss at the 1-th epoch 5500-th iteration is 10.673691988271809\n",
      "The loss at the 1-th epoch 5550-th iteration is 10.47313087041703\n",
      "The loss at the 1-th epoch 5600-th iteration is 10.263772802224832\n",
      "The loss at the 1-th epoch 5650-th iteration is 10.071865467623873\n",
      "The loss at the 1-th epoch 5700-th iteration is 9.883509575004979\n",
      "The loss at the 1-th epoch 5750-th iteration is 9.696190292292158\n",
      "The loss at the 1-th epoch 5800-th iteration is 9.505336123895077\n",
      "The loss at the 1-th epoch 5850-th iteration is 9.33329804588638\n",
      "The loss at the 1-th epoch 5900-th iteration is 9.172995433993528\n",
      "The loss at the 1-th epoch 5950-th iteration is 8.993337379331573\n",
      "The loss at the 1-th epoch 6000-th iteration is 8.81906481902334\n",
      "The loss at the 1-th epoch 6050-th iteration is 8.64980197037293\n",
      "The loss at the 1-th epoch 6100-th iteration is 8.492813500178903\n",
      "The loss at the 1-th epoch 6150-th iteration is 8.339925883555091\n",
      "The loss at the 1-th epoch 6200-th iteration is 8.187808331005657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss at the 1-th epoch 6250-th iteration is 8.041522170109072\n",
      "The loss at the 1-th epoch 6300-th iteration is 7.891760749979289\n",
      "The loss at the 1-th epoch 6350-th iteration is 7.7500328733347095\n",
      "The loss at the 1-th epoch 6400-th iteration is 7.595059447512895\n",
      "The loss at the 1-th epoch 6450-th iteration is 7.447557213422314\n",
      "The loss at the 1-th epoch 6500-th iteration is 7.308190636213456\n",
      "The loss at the 1-th epoch 6550-th iteration is 7.164489020486231\n",
      "The loss at the 1-th epoch 6600-th iteration is 7.0363655646265935\n",
      "The loss at the 1-th epoch 6650-th iteration is 6.903011353080766\n",
      "The loss at the 1-th epoch 6700-th iteration is 6.769852395625236\n",
      "The loss at the 1-th epoch 6750-th iteration is 6.639997681874158\n",
      "The loss at the 1-th epoch 6800-th iteration is 6.514822363096414\n",
      "The loss at the 1-th epoch 6850-th iteration is 6.396596135002473\n",
      "The loss at the 1-th epoch 6900-th iteration is 6.282411422803864\n",
      "The loss at the 1-th epoch 6950-th iteration is 6.164792359276955\n",
      "The loss at the 1-th epoch 7000-th iteration is 6.0458873330003176\n",
      "The loss at the 1-th epoch 7050-th iteration is 5.9264252671778515\n",
      "The loss at the 1-th epoch 7100-th iteration is 5.8272888008407095\n",
      "The loss at the 1-th epoch 7150-th iteration is 5.727781631116856\n",
      "The loss at the 1-th epoch 7200-th iteration is 5.625454460365231\n",
      "The loss at the 1-th epoch 7250-th iteration is 5.519362095657364\n",
      "The loss at the 1-th epoch 7300-th iteration is 5.417145250038464\n",
      "The loss at the 1-th epoch 7350-th iteration is 5.320933509078362\n",
      "The loss at the 1-th epoch 7400-th iteration is 5.228918317457216\n",
      "The loss at the 1-th epoch 7450-th iteration is 5.1423394923935835\n",
      "The loss at the 1-th epoch 7500-th iteration is 5.04534545028026\n",
      "The loss at the 1-th epoch 7550-th iteration is 4.95960670648652\n",
      "The loss at the 1-th epoch 7600-th iteration is 4.862549231169315\n",
      "The loss at the 1-th epoch 7650-th iteration is 4.778769774134492\n",
      "The loss at the 1-th epoch 7700-th iteration is 4.707594341455793\n",
      "The loss at the 1-th epoch 7750-th iteration is 4.624458998228793\n",
      "The loss at the 1-th epoch 7800-th iteration is 4.543919135936019\n",
      "The loss at the 1-th epoch 7850-th iteration is 4.466328153515998\n",
      "The loss at the 1-th epoch 7900-th iteration is 4.385281076225718\n",
      "The loss at the 1-th epoch 7950-th iteration is 4.308713450570739\n",
      "The loss at the 1-th epoch 8000-th iteration is 4.237732253435963\n",
      "The loss at the 1-th epoch 8050-th iteration is 4.164628199051529\n",
      "The loss at the 1-th epoch 8100-th iteration is 4.09375962455116\n",
      "The loss at the 1-th epoch 8150-th iteration is 4.026247740733671\n",
      "The loss at the 1-th epoch 8200-th iteration is 3.9537583548272517\n",
      "The loss at the 1-th epoch 8250-th iteration is 3.879709180445598\n",
      "The loss at the 1-th epoch 8300-th iteration is 3.818832140502627\n",
      "The loss at the 1-th epoch 8350-th iteration is 3.7514512156042508\n",
      "The loss at the 1-th epoch 8400-th iteration is 3.690629209020683\n",
      "The loss at the 1-th epoch 8450-th iteration is 3.6270276926699716\n",
      "The loss at the 1-th epoch 8500-th iteration is 3.5708172901566266\n",
      "The loss at the 1-th epoch 8550-th iteration is 3.5099735980894002\n",
      "The loss at the 1-th epoch 8600-th iteration is 3.452720929217409\n",
      "The loss at the 1-th epoch 8650-th iteration is 3.3900342618860986\n",
      "The loss at the 1-th epoch 8700-th iteration is 3.332498003930847\n",
      "The loss at the 1-th epoch 8750-th iteration is 3.2787463874038356\n",
      "The loss at the 1-th epoch 8800-th iteration is 3.221921956602058\n",
      "The loss at the 1-th epoch 8850-th iteration is 3.16826632231135\n",
      "The loss at the 1-th epoch 8900-th iteration is 3.112966608764384\n",
      "The loss at the 1-th epoch 8950-th iteration is 3.062461982796912\n",
      "The loss at the 1-th epoch 9000-th iteration is 3.017685784653863\n",
      "The loss at the 1-th epoch 9050-th iteration is 2.965160040484165\n",
      "The loss at the 1-th epoch 9100-th iteration is 2.9152750599543835\n",
      "The loss at the 1-th epoch 9150-th iteration is 2.8679540461185704\n",
      "The loss at the 1-th epoch 9200-th iteration is 2.829866676923039\n",
      "The loss at the 1-th epoch 9250-th iteration is 2.7844728538971304\n",
      "The loss at the 1-th epoch 9300-th iteration is 2.742588151321959\n",
      "The loss at the 1-th epoch 9350-th iteration is 2.700331946869687\n",
      "The loss at the 1-th epoch 9400-th iteration is 2.661670214362645\n",
      "The loss at the 1-th epoch 9450-th iteration is 2.6201716930112076\n",
      "The loss at the 1-th epoch 9500-th iteration is 2.581306573688596\n",
      "The loss at the 1-th epoch 9550-th iteration is 2.5420848993735583\n",
      "The loss at the 1-th epoch 9600-th iteration is 2.498186754329787\n",
      "The loss at the 1-th epoch 9650-th iteration is 2.463108376938225\n",
      "The loss at the 1-th epoch 9700-th iteration is 2.424497791602559\n",
      "The loss at the 1-th epoch 9750-th iteration is 2.3822312337085574\n",
      "The loss at the 1-th epoch 9800-th iteration is 2.3408440634340417\n",
      "The loss at the 1-th epoch 9850-th iteration is 2.3085608092311007\n",
      "The loss at the 1-th epoch 9900-th iteration is 2.270224634170943\n",
      "The loss at the 1-th epoch 9950-th iteration is 2.236758270077087\n",
      "Finished training 2-th epoch with total 10000 iterations\n",
      "The current learning rate is 0.0005\n",
      "The loss at the 2-th epoch 10000-th iteration is 2.2099494192913647\n",
      "The loss at the 2-th epoch 10050-th iteration is 2.176720651632954\n",
      "The loss at the 2-th epoch 10100-th iteration is 2.1480761442714034\n",
      "The loss at the 2-th epoch 10150-th iteration is 2.11841833828393\n",
      "The loss at the 2-th epoch 10200-th iteration is 2.0866060861554225\n",
      "The loss at the 2-th epoch 10250-th iteration is 2.0579504037592224\n",
      "The loss at the 2-th epoch 10300-th iteration is 2.028032663069626\n",
      "The loss at the 2-th epoch 10350-th iteration is 2.007484913058117\n",
      "The loss at the 2-th epoch 10400-th iteration is 1.9809118337486344\n",
      "The loss at the 2-th epoch 10450-th iteration is 1.954137229172911\n",
      "The loss at the 2-th epoch 10500-th iteration is 1.9288002130638762\n",
      "The loss at the 2-th epoch 10550-th iteration is 1.8998280843666977\n",
      "The loss at the 2-th epoch 10600-th iteration is 1.8727475459856449\n",
      "The loss at the 2-th epoch 10650-th iteration is 1.851635862482011\n",
      "The loss at the 2-th epoch 10700-th iteration is 1.8302544596886177\n",
      "The loss at the 2-th epoch 10750-th iteration is 1.800397882785192\n",
      "The loss at the 2-th epoch 10800-th iteration is 1.7762182553150838\n",
      "The loss at the 2-th epoch 10850-th iteration is 1.7467864913419038\n",
      "The loss at the 2-th epoch 10900-th iteration is 1.7246217412222582\n",
      "The loss at the 2-th epoch 10950-th iteration is 1.7025660830687612\n",
      "The loss at the 2-th epoch 11000-th iteration is 1.680997766632074\n",
      "The loss at the 2-th epoch 11050-th iteration is 1.6574005401848044\n",
      "The loss at the 2-th epoch 11100-th iteration is 1.6385556035593383\n",
      "The loss at the 2-th epoch 11150-th iteration is 1.617769889900925\n",
      "The loss at the 2-th epoch 11200-th iteration is 1.5978356645210754\n",
      "The loss at the 2-th epoch 11250-th iteration is 1.5808073582596265\n",
      "The loss at the 2-th epoch 11300-th iteration is 1.5651937748557552\n",
      "The loss at the 2-th epoch 11350-th iteration is 1.5459393307948108\n",
      "The loss at the 2-th epoch 11400-th iteration is 1.5245583123421684\n",
      "The loss at the 2-th epoch 11450-th iteration is 1.5030729693823803\n",
      "The loss at the 2-th epoch 11500-th iteration is 1.4804250630213782\n",
      "The loss at the 2-th epoch 11550-th iteration is 1.4619168166586851\n",
      "The loss at the 2-th epoch 11600-th iteration is 1.443412856624933\n",
      "The loss at the 2-th epoch 11650-th iteration is 1.4276408824369098\n",
      "The loss at the 2-th epoch 11700-th iteration is 1.4091657985713508\n",
      "The loss at the 2-th epoch 11750-th iteration is 1.389691735412171\n",
      "The loss at the 2-th epoch 11800-th iteration is 1.373215287845551\n",
      "The loss at the 2-th epoch 11850-th iteration is 1.355864882020063\n",
      "The loss at the 2-th epoch 11900-th iteration is 1.333340969564542\n",
      "The loss at the 2-th epoch 11950-th iteration is 1.3178292370693132\n",
      "The loss at the 2-th epoch 12000-th iteration is 1.2978043206424266\n",
      "The loss at the 2-th epoch 12050-th iteration is 1.281068594989902\n",
      "The loss at the 2-th epoch 12100-th iteration is 1.2654267982340113\n",
      "The loss at the 2-th epoch 12150-th iteration is 1.2477824720914477\n",
      "The loss at the 2-th epoch 12200-th iteration is 1.2326355846660482\n",
      "The loss at the 2-th epoch 12250-th iteration is 1.2167265551456108\n",
      "The loss at the 2-th epoch 12300-th iteration is 1.2017216613995356\n",
      "The loss at the 2-th epoch 12350-th iteration is 1.19067241170425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss at the 2-th epoch 12400-th iteration is 1.1779118211797839\n",
      "The loss at the 2-th epoch 12450-th iteration is 1.165894768933382\n",
      "The loss at the 2-th epoch 12500-th iteration is 1.1477085020667772\n",
      "The loss at the 2-th epoch 12550-th iteration is 1.1319470928897435\n",
      "The loss at the 2-th epoch 12600-th iteration is 1.114053313214196\n",
      "The loss at the 2-th epoch 12650-th iteration is 1.098601864343419\n",
      "The loss at the 2-th epoch 12700-th iteration is 1.083659943414652\n",
      "The loss at the 2-th epoch 12750-th iteration is 1.0728164895302585\n",
      "The loss at the 2-th epoch 12800-th iteration is 1.0557413322730844\n",
      "The loss at the 2-th epoch 12850-th iteration is 1.0466382901041267\n",
      "The loss at the 2-th epoch 12900-th iteration is 1.039640731270128\n",
      "The loss at the 2-th epoch 12950-th iteration is 1.0273027986048526\n",
      "The loss at the 2-th epoch 13000-th iteration is 1.0121528920741008\n",
      "The loss at the 2-th epoch 13050-th iteration is 0.999573675750703\n",
      "The loss at the 2-th epoch 13100-th iteration is 0.9878227979450975\n",
      "The loss at the 2-th epoch 13150-th iteration is 0.9702242240644009\n",
      "The loss at the 2-th epoch 13200-th iteration is 0.9583230970853694\n",
      "The loss at the 2-th epoch 13250-th iteration is 0.9450850195960239\n",
      "The loss at the 2-th epoch 13300-th iteration is 0.9266492043192964\n",
      "The loss at the 2-th epoch 13350-th iteration is 0.9131240749220647\n",
      "The loss at the 2-th epoch 13400-th iteration is 0.9028252921722536\n",
      "The loss at the 2-th epoch 13450-th iteration is 0.887895333995905\n",
      "The loss at the 2-th epoch 13500-th iteration is 0.8731405163842552\n",
      "The loss at the 2-th epoch 13550-th iteration is 0.8623849588791027\n",
      "The loss at the 2-th epoch 13600-th iteration is 0.8490361475623436\n",
      "The loss at the 2-th epoch 13650-th iteration is 0.8372220427771409\n",
      "The loss at the 2-th epoch 13700-th iteration is 0.8257527966937217\n",
      "The loss at the 2-th epoch 13750-th iteration is 0.8131100920304997\n",
      "The loss at the 2-th epoch 13800-th iteration is 0.7999160247074287\n",
      "The loss at the 2-th epoch 13850-th iteration is 0.7880224280808212\n",
      "The loss at the 2-th epoch 13900-th iteration is 0.7765676011189127\n",
      "The loss at the 2-th epoch 13950-th iteration is 0.7645990296978148\n",
      "The loss at the 2-th epoch 14000-th iteration is 0.755682682810599\n",
      "The loss at the 2-th epoch 14050-th iteration is 0.7461083462206912\n",
      "The loss at the 2-th epoch 14100-th iteration is 0.7325097857227313\n",
      "The loss at the 2-th epoch 14150-th iteration is 0.7183190937915959\n",
      "The loss at the 2-th epoch 14200-th iteration is 0.7067839687523582\n",
      "The loss at the 2-th epoch 14250-th iteration is 0.6937397087007341\n",
      "The loss at the 2-th epoch 14300-th iteration is 0.6852370619751218\n",
      "The loss at the 2-th epoch 14350-th iteration is 0.6758719368105578\n",
      "The loss at the 2-th epoch 14400-th iteration is 0.6685151025488976\n",
      "The loss at the 2-th epoch 14450-th iteration is 0.6603438251775618\n",
      "The loss at the 2-th epoch 14500-th iteration is 0.6502074131840262\n",
      "The loss at the 2-th epoch 14550-th iteration is 0.6433819719478643\n",
      "The loss at the 2-th epoch 14600-th iteration is 0.635839067451282\n",
      "The loss at the 2-th epoch 14650-th iteration is 0.6293762191789238\n",
      "The loss at the 2-th epoch 14700-th iteration is 0.6174904955932202\n",
      "The loss at the 2-th epoch 14750-th iteration is 0.6105925567502497\n",
      "The loss at the 2-th epoch 14800-th iteration is 0.6042692176599492\n",
      "The loss at the 2-th epoch 14850-th iteration is 0.5977310441164645\n",
      "The loss at the 2-th epoch 14900-th iteration is 0.5903175499601705\n",
      "The loss at the 2-th epoch 14950-th iteration is 0.5839402718653977\n",
      "Finished training 3-th epoch with total 15000 iterations\n",
      "The current learning rate is 0.0005\n",
      "The loss at the 3-th epoch 15000-th iteration is 0.5795804357900791\n",
      "The loss at the 3-th epoch 15050-th iteration is 0.573542771637871\n",
      "The loss at the 3-th epoch 15100-th iteration is 0.5688152249950069\n",
      "The loss at the 3-th epoch 15150-th iteration is 0.5633899071124221\n",
      "The loss at the 3-th epoch 15200-th iteration is 0.5568755484944563\n",
      "The loss at the 3-th epoch 15250-th iteration is 0.5502650422123377\n",
      "The loss at the 3-th epoch 15300-th iteration is 0.544851193850984\n",
      "The loss at the 3-th epoch 15350-th iteration is 0.5411186914638672\n",
      "The loss at the 3-th epoch 15400-th iteration is 0.5359503088276537\n",
      "The loss at the 3-th epoch 15450-th iteration is 0.5314480355431863\n",
      "The loss at the 3-th epoch 15500-th iteration is 0.5280865686567928\n",
      "The loss at the 3-th epoch 15550-th iteration is 0.5243507977524706\n",
      "The loss at the 3-th epoch 15600-th iteration is 0.5197328533198015\n",
      "The loss at the 3-th epoch 15650-th iteration is 0.5185307698876552\n",
      "The loss at the 3-th epoch 15700-th iteration is 0.5157309728205721\n",
      "The loss at the 3-th epoch 15750-th iteration is 0.5117080500802581\n",
      "The loss at the 3-th epoch 15800-th iteration is 0.5081834935127633\n",
      "The loss at the 3-th epoch 15850-th iteration is 0.5027373728176315\n",
      "The loss at the 3-th epoch 15900-th iteration is 0.4989374089170676\n",
      "The loss at the 3-th epoch 15950-th iteration is 0.49464362584929633\n",
      "The loss at the 3-th epoch 16000-th iteration is 0.4914688546712528\n",
      "The loss at the 3-th epoch 16050-th iteration is 0.4877296945164795\n",
      "The loss at the 3-th epoch 16100-th iteration is 0.48533101540098167\n",
      "The loss at the 3-th epoch 16150-th iteration is 0.4824120708536499\n",
      "The loss at the 3-th epoch 16200-th iteration is 0.4773878511066906\n",
      "The loss at the 3-th epoch 16250-th iteration is 0.4763871736800649\n",
      "The loss at the 3-th epoch 16300-th iteration is 0.4746123767612225\n",
      "The loss at the 3-th epoch 16350-th iteration is 0.47113213384152464\n",
      "The loss at the 3-th epoch 16400-th iteration is 0.4676270816509923\n",
      "The loss at the 3-th epoch 16450-th iteration is 0.46524271263669004\n",
      "The loss at the 3-th epoch 16500-th iteration is 0.462932096414327\n",
      "The loss at the 3-th epoch 16550-th iteration is 0.4601820551745244\n",
      "The loss at the 3-th epoch 16600-th iteration is 0.4574783321748651\n",
      "The loss at the 3-th epoch 16650-th iteration is 0.45356760448247\n",
      "The loss at the 3-th epoch 16700-th iteration is 0.4495349099012039\n",
      "The loss at the 3-th epoch 16750-th iteration is 0.4461904630355059\n",
      "The loss at the 3-th epoch 16800-th iteration is 0.44443203506632134\n",
      "The loss at the 3-th epoch 16850-th iteration is 0.44232625064964626\n",
      "The loss at the 3-th epoch 16900-th iteration is 0.43964130907261056\n",
      "The loss at the 3-th epoch 16950-th iteration is 0.43923260992313373\n",
      "The loss at the 3-th epoch 17000-th iteration is 0.4363501741628646\n",
      "The loss at the 3-th epoch 17050-th iteration is 0.43375381090764165\n",
      "The loss at the 3-th epoch 17100-th iteration is 0.43283659412334097\n",
      "The loss at the 3-th epoch 17150-th iteration is 0.4317683116080294\n",
      "The loss at the 3-th epoch 17200-th iteration is 0.4294343348448528\n",
      "The loss at the 3-th epoch 17250-th iteration is 0.4268756919521558\n",
      "The loss at the 3-th epoch 17300-th iteration is 0.42598514613771676\n",
      "The loss at the 3-th epoch 17350-th iteration is 0.42464455186670724\n",
      "The loss at the 3-th epoch 17400-th iteration is 0.423942472352025\n",
      "The loss at the 3-th epoch 17450-th iteration is 0.4257768563495101\n",
      "The loss at the 3-th epoch 17500-th iteration is 0.4217839879266453\n",
      "The loss at the 3-th epoch 17550-th iteration is 0.42163623519923044\n",
      "The loss at the 3-th epoch 17600-th iteration is 0.41983364540596824\n",
      "The loss at the 3-th epoch 17650-th iteration is 0.42087439374431546\n",
      "The loss at the 3-th epoch 17700-th iteration is 0.42078046518265344\n",
      "The loss at the 3-th epoch 17750-th iteration is 0.42096300592114944\n",
      "The loss at the 3-th epoch 17800-th iteration is 0.41863443261502387\n",
      "The loss at the 3-th epoch 17850-th iteration is 0.41955154065113076\n",
      "The loss at the 3-th epoch 17900-th iteration is 0.4208112879170167\n",
      "The loss at the 3-th epoch 17950-th iteration is 0.42023252811451584\n",
      "The loss at the 3-th epoch 18000-th iteration is 0.41840690820367044\n",
      "The loss at the 3-th epoch 18050-th iteration is 0.41758847102471786\n",
      "The loss at the 3-th epoch 18100-th iteration is 0.4168106300546389\n",
      "The loss at the 3-th epoch 18150-th iteration is 0.41560835168620436\n",
      "The loss at the 3-th epoch 18200-th iteration is 0.41541576198770014\n",
      "The loss at the 3-th epoch 18250-th iteration is 0.415363682669755\n",
      "The loss at the 3-th epoch 18300-th iteration is 0.4152447601956657\n",
      "The loss at the 3-th epoch 18350-th iteration is 0.4142408049031726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss at the 3-th epoch 18400-th iteration is 0.41367599260433185\n",
      "The loss at the 3-th epoch 18450-th iteration is 0.41185031702229724\n",
      "The loss at the 3-th epoch 18500-th iteration is 0.41203645210077855\n",
      "The loss at the 3-th epoch 18550-th iteration is 0.4115562567545672\n",
      "The loss at the 3-th epoch 18600-th iteration is 0.41231672813520626\n",
      "The loss at the 3-th epoch 18650-th iteration is 0.41119040617462277\n",
      "The loss at the 3-th epoch 18700-th iteration is 0.41147143250923934\n",
      "The loss at the 3-th epoch 18750-th iteration is 0.4117860283030257\n",
      "The loss at the 3-th epoch 18800-th iteration is 0.4104746421938932\n",
      "The loss at the 3-th epoch 18850-th iteration is 0.41115778569007755\n",
      "The loss at the 3-th epoch 18900-th iteration is 0.4102744156795676\n",
      "The loss at the 3-th epoch 18950-th iteration is 0.41042023308856546\n",
      "The loss at the 3-th epoch 19000-th iteration is 0.41037335986020396\n",
      "The loss at the 3-th epoch 19050-th iteration is 0.4112402914359453\n",
      "The loss at the 3-th epoch 19100-th iteration is 0.4094065025980964\n",
      "The loss at the 3-th epoch 19150-th iteration is 0.4073790386099144\n",
      "The loss at the 3-th epoch 19200-th iteration is 0.4054673879956572\n",
      "The loss at the 3-th epoch 19250-th iteration is 0.4040630602998948\n",
      "The loss at the 3-th epoch 19300-th iteration is 0.40450815904301884\n",
      "The loss at the 3-th epoch 19350-th iteration is 0.4038080602301073\n",
      "The loss at the 3-th epoch 19400-th iteration is 0.4039288349067516\n",
      "The loss at the 3-th epoch 19450-th iteration is 0.403281925113243\n",
      "The loss at the 3-th epoch 19500-th iteration is 0.40262333563645436\n",
      "The loss at the 3-th epoch 19550-th iteration is 0.4038625065151322\n",
      "The loss at the 3-th epoch 19600-th iteration is 0.40281235276898597\n",
      "The loss at the 3-th epoch 19650-th iteration is 0.40276003959971995\n",
      "The loss at the 3-th epoch 19700-th iteration is 0.4018618105415248\n",
      "The loss at the 3-th epoch 19750-th iteration is 0.4005405403678123\n",
      "The loss at the 3-th epoch 19800-th iteration is 0.3999157195936197\n",
      "The loss at the 3-th epoch 19850-th iteration is 0.3994412810315806\n",
      "The loss at the 3-th epoch 19900-th iteration is 0.397806393890497\n",
      "The loss at the 3-th epoch 19950-th iteration is 0.3991664849922959\n",
      "Finished training 4-th epoch with total 20000 iterations\n",
      "The current learning rate is 0.0005\n",
      "The loss at the 4-th epoch 20000-th iteration is 0.3991436465284426\n",
      "The loss at the 4-th epoch 20050-th iteration is 0.399262503882453\n",
      "The loss at the 4-th epoch 20100-th iteration is 0.39915450250876594\n",
      "The loss at the 4-th epoch 20150-th iteration is 0.39895925970748136\n",
      "The loss at the 4-th epoch 20200-th iteration is 0.3985525906318821\n",
      "The loss at the 4-th epoch 20250-th iteration is 0.39927015933495646\n",
      "The loss at the 4-th epoch 20300-th iteration is 0.39896101937744144\n",
      "The loss at the 4-th epoch 20350-th iteration is 0.4005089734111509\n",
      "The loss at the 4-th epoch 20400-th iteration is 0.3990522607162027\n",
      "The loss at the 4-th epoch 20450-th iteration is 0.39915505303013976\n",
      "The loss at the 4-th epoch 20500-th iteration is 0.40065949328716044\n",
      "The loss at the 4-th epoch 20550-th iteration is 0.40065409961245557\n",
      "The loss at the 4-th epoch 20600-th iteration is 0.40104393454996956\n",
      "The loss at the 4-th epoch 20650-th iteration is 0.4030803833368857\n",
      "The loss at the 4-th epoch 20700-th iteration is 0.40418104155885803\n",
      "The loss at the 4-th epoch 20750-th iteration is 0.4045544076248502\n",
      "The loss at the 4-th epoch 20800-th iteration is 0.40572815023505715\n",
      "The loss at the 4-th epoch 20850-th iteration is 0.404826363034394\n",
      "The loss at the 4-th epoch 20900-th iteration is 0.40444110431644376\n",
      "The loss at the 4-th epoch 20950-th iteration is 0.4027902147254584\n",
      "The loss at the 4-th epoch 21000-th iteration is 0.40333288597557293\n",
      "The loss at the 4-th epoch 21050-th iteration is 0.4020715031476032\n",
      "The loss at the 4-th epoch 21100-th iteration is 0.40241719139282894\n",
      "The loss at the 4-th epoch 21150-th iteration is 0.40268280422473113\n",
      "The loss at the 4-th epoch 21200-th iteration is 0.4014098613771876\n",
      "The loss at the 4-th epoch 21250-th iteration is 0.40285933228778503\n",
      "The loss at the 4-th epoch 21300-th iteration is 0.4038848279225338\n",
      "The loss at the 4-th epoch 21350-th iteration is 0.40225913766337573\n",
      "The loss at the 4-th epoch 21400-th iteration is 0.40143401565945935\n",
      "The loss at the 4-th epoch 21450-th iteration is 0.40076423027380725\n",
      "The loss at the 4-th epoch 21500-th iteration is 0.4016323925464613\n",
      "The loss at the 4-th epoch 21550-th iteration is 0.40119152218186993\n",
      "The loss at the 4-th epoch 21600-th iteration is 0.40077351682326245\n",
      "The loss at the 4-th epoch 21650-th iteration is 0.39902818229872905\n",
      "The loss at the 4-th epoch 21700-th iteration is 0.3965190395883844\n",
      "The loss at the 4-th epoch 21750-th iteration is 0.39551139025067694\n",
      "The loss at the 4-th epoch 21800-th iteration is 0.3950377303982743\n",
      "The loss at the 4-th epoch 21850-th iteration is 0.39547014426841987\n",
      "The loss at the 4-th epoch 21900-th iteration is 0.3959614015099835\n",
      "The loss at the 4-th epoch 21950-th iteration is 0.39652887333113573\n",
      "The loss at the 4-th epoch 22000-th iteration is 0.3954329647336946\n",
      "The loss at the 4-th epoch 22050-th iteration is 0.3941729955874377\n",
      "The loss at the 4-th epoch 22100-th iteration is 0.39455822064595925\n",
      "The loss at the 4-th epoch 22150-th iteration is 0.3951379856994277\n",
      "The loss at the 4-th epoch 22200-th iteration is 0.39384420908304696\n",
      "The loss at the 4-th epoch 22250-th iteration is 0.3924793155790438\n",
      "The loss at the 4-th epoch 22300-th iteration is 0.3928797437015449\n",
      "The loss at the 4-th epoch 22350-th iteration is 0.3926612300387444\n",
      "The loss at the 4-th epoch 22400-th iteration is 0.39276860362783095\n",
      "The loss at the 4-th epoch 22450-th iteration is 0.39539862409465054\n",
      "The loss at the 4-th epoch 22500-th iteration is 0.392733954576438\n",
      "The loss at the 4-th epoch 22550-th iteration is 0.39413769925839803\n",
      "The loss at the 4-th epoch 22600-th iteration is 0.3933940214430882\n",
      "The loss at the 4-th epoch 22650-th iteration is 0.395452846372364\n",
      "The loss at the 4-th epoch 22700-th iteration is 0.3971979079533944\n",
      "The loss at the 4-th epoch 22750-th iteration is 0.3982984908466811\n",
      "The loss at the 4-th epoch 22800-th iteration is 0.39721781159565306\n",
      "The loss at the 4-th epoch 22850-th iteration is 0.3986393343656278\n",
      "The loss at the 4-th epoch 22900-th iteration is 0.40024946885351453\n",
      "The loss at the 4-th epoch 22950-th iteration is 0.4002833198157193\n",
      "The loss at the 4-th epoch 23000-th iteration is 0.39917019707138557\n",
      "The loss at the 4-th epoch 23050-th iteration is 0.39909849128613406\n",
      "The loss at the 4-th epoch 23100-th iteration is 0.3989668189641744\n",
      "The loss at the 4-th epoch 23150-th iteration is 0.3987734745854531\n",
      "The loss at the 4-th epoch 23200-th iteration is 0.3993209520776615\n",
      "The loss at the 4-th epoch 23250-th iteration is 0.40006457816146035\n",
      "The loss at the 4-th epoch 23300-th iteration is 0.4007976777676291\n",
      "The loss at the 4-th epoch 23350-th iteration is 0.400215603402162\n",
      "The loss at the 4-th epoch 23400-th iteration is 0.3999077642805253\n",
      "The loss at the 4-th epoch 23450-th iteration is 0.39857650512277254\n",
      "The loss at the 4-th epoch 23500-th iteration is 0.39943261275048925\n",
      "The loss at the 4-th epoch 23550-th iteration is 0.39926666376642006\n",
      "The loss at the 4-th epoch 23600-th iteration is 0.4004930193081726\n",
      "The loss at the 4-th epoch 23650-th iteration is 0.3997381882320671\n",
      "The loss at the 4-th epoch 23700-th iteration is 0.40042077270654153\n",
      "The loss at the 4-th epoch 23750-th iteration is 0.4014185499281743\n",
      "The loss at the 4-th epoch 23800-th iteration is 0.40065505931621603\n",
      "The loss at the 4-th epoch 23850-th iteration is 0.40202044205915477\n",
      "The loss at the 4-th epoch 23900-th iteration is 0.40154703058831964\n",
      "The loss at the 4-th epoch 23950-th iteration is 0.4020980445456087\n",
      "The loss at the 4-th epoch 24000-th iteration is 0.40247419647107174\n",
      "The loss at the 4-th epoch 24050-th iteration is 0.4036481369022562\n",
      "The loss at the 4-th epoch 24100-th iteration is 0.4022844523912814\n",
      "The loss at the 4-th epoch 24150-th iteration is 0.4006002113882514\n",
      "The loss at the 4-th epoch 24200-th iteration is 0.3990653141316409\n",
      "The loss at the 4-th epoch 24250-th iteration is 0.3981851866730943\n",
      "The loss at the 4-th epoch 24300-th iteration is 0.39869505962648855\n",
      "The loss at the 4-th epoch 24350-th iteration is 0.398193942651062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss at the 4-th epoch 24400-th iteration is 0.39853688198753123\n",
      "The loss at the 4-th epoch 24450-th iteration is 0.39818064931730013\n",
      "The loss at the 4-th epoch 24500-th iteration is 0.3977971254786731\n",
      "The loss at the 4-th epoch 24550-th iteration is 0.3992636559060907\n",
      "The loss at the 4-th epoch 24600-th iteration is 0.3981148387952822\n",
      "The loss at the 4-th epoch 24650-th iteration is 0.3981411663530996\n",
      "The loss at the 4-th epoch 24700-th iteration is 0.3973579048877679\n",
      "The loss at the 4-th epoch 24750-th iteration is 0.39617994368379317\n",
      "The loss at the 4-th epoch 24800-th iteration is 0.3956487044779372\n",
      "The loss at the 4-th epoch 24850-th iteration is 0.3953548147435323\n",
      "The loss at the 4-th epoch 24900-th iteration is 0.393990277333522\n",
      "The loss at the 4-th epoch 24950-th iteration is 0.3954737071497195\n",
      "Finished training 5-th epoch with total 25000 iterations\n",
      "The current learning rate is 0.0005\n",
      "The loss at the 5-th epoch 25000-th iteration is 0.39557253843857954\n",
      "The loss at the 5-th epoch 25050-th iteration is 0.3958854654962005\n",
      "The loss at the 5-th epoch 25100-th iteration is 0.3958642302236395\n",
      "The loss at the 5-th epoch 25150-th iteration is 0.39564894650821003\n",
      "The loss at the 5-th epoch 25200-th iteration is 0.3953795645242143\n",
      "The loss at the 5-th epoch 25250-th iteration is 0.39634265123406254\n",
      "The loss at the 5-th epoch 25300-th iteration is 0.39621978664287083\n",
      "The loss at the 5-th epoch 25350-th iteration is 0.39801253686247673\n",
      "The loss at the 5-th epoch 25400-th iteration is 0.3965524659470162\n",
      "The loss at the 5-th epoch 25450-th iteration is 0.3967781323907401\n",
      "The loss at the 5-th epoch 25500-th iteration is 0.39844840215971444\n",
      "The loss at the 5-th epoch 25550-th iteration is 0.39842138519951553\n",
      "The loss at the 5-th epoch 25600-th iteration is 0.39893556060741653\n",
      "The loss at the 5-th epoch 25650-th iteration is 0.4009907825522695\n",
      "The loss at the 5-th epoch 25700-th iteration is 0.40220594406658083\n",
      "The loss at the 5-th epoch 25750-th iteration is 0.40263822952368544\n",
      "The loss at the 5-th epoch 25800-th iteration is 0.40398791703719483\n",
      "The loss at the 5-th epoch 25850-th iteration is 0.4030879807811063\n",
      "The loss at the 5-th epoch 25900-th iteration is 0.4027481406329907\n",
      "The loss at the 5-th epoch 25950-th iteration is 0.40117363005646134\n",
      "The loss at the 5-th epoch 26000-th iteration is 0.4018464183068793\n",
      "The loss at the 5-th epoch 26050-th iteration is 0.40059238730741065\n",
      "The loss at the 5-th epoch 26100-th iteration is 0.4009409816621161\n",
      "The loss at the 5-th epoch 26150-th iteration is 0.40126736516190853\n",
      "The loss at the 5-th epoch 26200-th iteration is 0.4001394888542449\n",
      "The loss at the 5-th epoch 26250-th iteration is 0.4015701353730361\n",
      "The loss at the 5-th epoch 26300-th iteration is 0.4026459698799193\n",
      "The loss at the 5-th epoch 26350-th iteration is 0.4009888992473015\n",
      "The loss at the 5-th epoch 26400-th iteration is 0.4002550293904503\n",
      "The loss at the 5-th epoch 26450-th iteration is 0.3995377372031793\n",
      "The loss at the 5-th epoch 26500-th iteration is 0.40048884861031664\n",
      "The loss at the 5-th epoch 26550-th iteration is 0.40005971814032293\n",
      "The loss at the 5-th epoch 26600-th iteration is 0.39964243009233247\n",
      "The loss at the 5-th epoch 26650-th iteration is 0.3980189148997175\n",
      "The loss at the 5-th epoch 26700-th iteration is 0.39549484340971086\n",
      "The loss at the 5-th epoch 26750-th iteration is 0.3945681850669801\n",
      "The loss at the 5-th epoch 26800-th iteration is 0.394095985264603\n",
      "The loss at the 5-th epoch 26850-th iteration is 0.39464206204014773\n",
      "The loss at the 5-th epoch 26900-th iteration is 0.3952025166760321\n",
      "The loss at the 5-th epoch 26950-th iteration is 0.3957364180555948\n",
      "The loss at the 5-th epoch 27000-th iteration is 0.3946725464170924\n",
      "The loss at the 5-th epoch 27050-th iteration is 0.3934246937429873\n",
      "The loss at the 5-th epoch 27100-th iteration is 0.3938357587953518\n",
      "The loss at the 5-th epoch 27150-th iteration is 0.3944244868153832\n",
      "The loss at the 5-th epoch 27200-th iteration is 0.39313332410743906\n",
      "The loss at the 5-th epoch 27250-th iteration is 0.3918250622886016\n",
      "The loss at the 5-th epoch 27300-th iteration is 0.3922561839437808\n",
      "The loss at the 5-th epoch 27350-th iteration is 0.392094728159907\n",
      "The loss at the 5-th epoch 27400-th iteration is 0.39219613709598156\n",
      "The loss at the 5-th epoch 27450-th iteration is 0.39480233102016105\n",
      "The loss at the 5-th epoch 27500-th iteration is 0.39218419418715356\n",
      "The loss at the 5-th epoch 27550-th iteration is 0.3936453856548535\n",
      "The loss at the 5-th epoch 27600-th iteration is 0.3928993752125035\n",
      "The loss at the 5-th epoch 27650-th iteration is 0.394944082651694\n",
      "The loss at the 5-th epoch 27700-th iteration is 0.3968095744872079\n",
      "The loss at the 5-th epoch 27750-th iteration is 0.39795325523892744\n",
      "The loss at the 5-th epoch 27800-th iteration is 0.3969201745722474\n",
      "The loss at the 5-th epoch 27850-th iteration is 0.3983422903194831\n",
      "The loss at the 5-th epoch 27900-th iteration is 0.39994062398247987\n",
      "The loss at the 5-th epoch 27950-th iteration is 0.3999666881575209\n",
      "The loss at the 5-th epoch 28000-th iteration is 0.39884046962204256\n",
      "The loss at the 5-th epoch 28050-th iteration is 0.398793972783795\n",
      "The loss at the 5-th epoch 28100-th iteration is 0.39868805343678815\n",
      "The loss at the 5-th epoch 28150-th iteration is 0.39851411667206094\n",
      "The loss at the 5-th epoch 28200-th iteration is 0.3990954707278792\n",
      "The loss at the 5-th epoch 28250-th iteration is 0.3998766091460063\n",
      "The loss at the 5-th epoch 28300-th iteration is 0.40060594044699227\n",
      "The loss at the 5-th epoch 28350-th iteration is 0.4000168064695183\n",
      "The loss at the 5-th epoch 28400-th iteration is 0.39968881775198517\n",
      "The loss at the 5-th epoch 28450-th iteration is 0.39835304153757867\n",
      "The loss at the 5-th epoch 28500-th iteration is 0.39922211204317193\n",
      "The loss at the 5-th epoch 28550-th iteration is 0.39905540268029094\n",
      "The loss at the 5-th epoch 28600-th iteration is 0.40026659942789256\n",
      "The loss at the 5-th epoch 28650-th iteration is 0.39951742169937854\n",
      "The loss at the 5-th epoch 28700-th iteration is 0.4001974917743899\n",
      "The loss at the 5-th epoch 28750-th iteration is 0.40123112401654304\n",
      "The loss at the 5-th epoch 28800-th iteration is 0.4004929065361036\n",
      "The loss at the 5-th epoch 28850-th iteration is 0.40189649276827877\n",
      "The loss at the 5-th epoch 28900-th iteration is 0.40143642948486635\n",
      "The loss at the 5-th epoch 28950-th iteration is 0.4019952933989346\n",
      "The loss at the 5-th epoch 29000-th iteration is 0.40239357126246045\n",
      "The loss at the 5-th epoch 29050-th iteration is 0.40356351929121714\n",
      "The loss at the 5-th epoch 29100-th iteration is 0.4022198744430242\n",
      "The loss at the 5-th epoch 29150-th iteration is 0.40054082975255034\n",
      "The loss at the 5-th epoch 29200-th iteration is 0.3990258785089491\n",
      "The loss at the 5-th epoch 29250-th iteration is 0.39817625739428075\n",
      "The loss at the 5-th epoch 29300-th iteration is 0.3986621039464644\n",
      "The loss at the 5-th epoch 29350-th iteration is 0.3981567190430082\n",
      "The loss at the 5-th epoch 29400-th iteration is 0.39850658687438556\n",
      "The loss at the 5-th epoch 29450-th iteration is 0.3981655229300396\n",
      "The loss at the 5-th epoch 29500-th iteration is 0.3977895719092285\n",
      "The loss at the 5-th epoch 29550-th iteration is 0.39926126746798807\n",
      "The loss at the 5-th epoch 29600-th iteration is 0.3980760044875401\n",
      "The loss at the 5-th epoch 29650-th iteration is 0.39809206086408844\n",
      "The loss at the 5-th epoch 29700-th iteration is 0.39728963129209116\n",
      "The loss at the 5-th epoch 29750-th iteration is 0.3961145581471748\n",
      "The loss at the 5-th epoch 29800-th iteration is 0.39557929758959465\n",
      "The loss at the 5-th epoch 29850-th iteration is 0.3952906429588471\n",
      "The loss at the 5-th epoch 29900-th iteration is 0.393949181200851\n",
      "The loss at the 5-th epoch 29950-th iteration is 0.3954249651682993\n",
      "Finished training 6-th epoch with total 30000 iterations\n",
      "The current learning rate is 0.0005\n",
      "The loss at the 6-th epoch 30000-th iteration is 0.39552863536852434\n"
     ]
    }
   ],
   "source": [
    "algo._train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_list.append(algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-8\n",
    "#iterations = range(int(2*T/ratio)+1)\n",
    "#frac_iterations = [ratio * t for t in iterations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_error_list = []\n",
    "error_list = []\n",
    "for trained_algo in algo_list:\n",
    "    #log_iterations = [math.log(t+1) for t in iterations]\n",
    "    log_error_list.append([math.log(y+eps) for y in trained_algo._groundtruth_eval_log])\n",
    "    error_list.append(trained_algo._groundtruth_eval_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx, error_log in enumerate(log_error_list):\n",
    "    number_of_iters = len(error_log)\n",
    "    frac_iterations = [ratio * t for t in range(number_of_iters)]\n",
    "    plt.plot(frac_iterations, error_log, label=f'noise-over-signal={perturb_ratio[idx]}')\n",
    "    plt.legend()\n",
    "    plt.xlabel('fraction of epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title(f'k={k}, d={d}')\n",
    "    plt.savefig(f'k{k}_d{d}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx, log_error in enumerate(log_error_list):\n",
    "    number_of_iters = len(log_error)\n",
    "    frac_iterations = [ratio * t for t in range(number_of_iters)]\n",
    "    plt.plot(frac_iterations, log_error, '--', label=f'noise-over-signal={perturb_ratio[idx]}')\n",
    "    plt.legend()\n",
    "    plt.xlabel('fraction of epoch')\n",
    "    plt.ylabel('log loss')\n",
    "    plt.title(f'k={k}, d={d}')\n",
    "    plt.savefig(f'k{k}_d{d}_logloss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
